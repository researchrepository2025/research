# F48 — On-Premises CI/CD Pipeline Infrastructure

**Research Question:** What is required to self-host a complete CI/CD pipeline infrastructure on-premises, and how do the operational characteristics and trade-offs compare to managed CI/CD services?

---

## Executive Summary

Self-hosting a complete CI/CD pipeline on-premises demands a stack of at least five distinct services — CI server, container registry, artifact repository, build agent fleet, and test infrastructure — each requiring independent installation, hardening, and ongoing maintenance. Jenkins alone published [eight or more distinct security advisories across 2025](https://www.jenkins.io/security/advisories/), illustrating the continuous patching burden teams absorb in lieu of a vendor. Managed services such as GitHub Actions, Azure DevOps Pipelines, and Google Cloud Build eliminate virtually all infrastructure management overhead at the cost of data-residency flexibility and, at scale, per-minute compute charges. For ISVs serving regulated verticals or customers with air-gapped requirements, on-premises CI/CD is non-negotiable but requires a minimum of 2.0–3.5 dedicated FTE to operate reliably. Managed Kubernetes (EKS/AKS/GKE) with self-hosted runners represents a middle path: it preserves data sovereignty and custom hardware access while offloading cluster-plane management to the provider.

---

## 1. CI Server Management

### 1.1 Jenkins

Jenkins is the dominant self-hosted CI server with over 1,800 plugins. It is described as "a highly scalable but operationally intensive platform that requires configuration management tools like Ansible, Puppet, or Chef to manage infrastructure" and often requires "dedicated engineers or Jenkins specialists to maintain the environment, plugins, and reliability." [Source: Aziro blog, Jenkins vs GitLab CI/CD comparison](https://www.aziro.com/en/blog/jenkins-vs-gitlab-vs-circleci-which-ci-cd-tool-is-right-for-you)

**Security patching cadence (2025):** The Jenkins project published security advisories on the following dates in 2025 alone: [January 22](https://www.jenkins.io/security/advisory/2025-01-22/), [March 5](https://www.jenkins.io/security/advisory/2025-03-05/), [April 2](https://www.jenkins.io/security/advisory/2025-04-02/), [May 14](https://www.jenkins.io/security/advisory/2025-05-14/), [July 9](https://www.jenkins.io/security/advisory/2025-07-09/), [September 3](https://www.jenkins.io/security/advisory/2025-09-03/), [September 17](https://www.jenkins.io/security/advisory/2025-09-17/), [October 29](https://www.jenkins.io/security/advisory/2025-10-29/), and [December 10](https://www.jenkins.io/security/advisory/2025-12-10/). That is nine advisories in a single calendar year. Common vulnerability types span "credential context validation, script command injection, and permission check failures" across both core and community plugins. [Source: CyberPress, Jenkins Plugin Vulnerabilities](https://cyberpress.org/jenkins-plugin-vulnerabilities/)

**Scaling runners:** Jenkins uses agents (workers) connected to the controller via JNLP or SSH. Scaling is manual or scripted via the Kubernetes plugin, which provisions pods dynamically. Each additional scaling tier introduces dependency on container orchestration expertise.

### 1.2 GitLab CI (Self-Managed)

GitLab CI runners are lightweight agents that "can run on Docker containers, virtual machines, and Kubernetes clusters, and support parallel execution." [Source: Aziro blog](https://www.aziro.com/en/blog/jenkins-vs-gitlab-vs-circleci-which-ci-cd-tool-is-right-for-you)

With the Kubernetes executor, each CI job runs in an isolated pod. The [GitLab Runner Fleet Scaling documentation](https://docs.gitlab.com/runner/fleet_scaling/) requires a minimum of two runner managers for autoscaling configurations, with total manager count depending on "concurrency configured for each manager and the load generated by CI/CD jobs hourly, daily, and monthly."

Operational overhead for GitLab self-managed includes "maintaining servers, backups, and security patches" requiring a dedicated team, plus Active Directory/LDAP integration for authentication. [Source: Aziro comparison blog](https://www.aziro.com/en/blog/jenkins-vs-gitlab-vs-circleci-which-ci-cd-tool-is-right-for-you)

A practical optimization path documented by SeatGeek identifies five critical improvement areas: "caching, autoscaling, bin packing, NVMe disk usage, and capacity reservation" — each requiring engineering time to tune. [Source: ChairNerd/SeatGeek, CI Runner Optimizations](https://chairnerd.seatgeek.com/ci-runner-optimizations/)

### 1.3 Woodpecker CI

Woodpecker CI is an open-source, container-native CI engine forked from Drone CI. At idle, "it requires around 100 MB of RAM (Server) and 30 MB (Agent)," contrasted with Jenkins' Java-based heap requirements. [Source: Woodpecker CI official site](https://woodpecker-ci.org/)

Woodpecker supports ephemeral runners and connects to GitLab, GitHub, Gitea, and Forgejo via OAuth. Its plugin model uses Docker container images, making plugin authoring simpler than Jenkins' JVM plugin architecture. [Source: GitHub, woodpecker-ci/woodpecker](https://github.com/woodpecker-ci/woodpecker)

One engineering team documented switching from GitHub Actions to Woodpecker CI, citing simpler pipeline YAML, lower resource footprint, and full data residency. [Source: Kalvad blog, Breaking Up with GitHub Actions](https://blog.kalvad.com/breaking-up-with-github-actions-our-love-story-with-woodpecker-ci/)

---

## 2. Container Registry: Harbor

### 2.1 Installation and Minimum Requirements

Harbor is a CNCF-graduated, open-source container registry. Minimum production hardware: "a Linux VM (Ubuntu or CentOS recommended) with at least 2 vCPUs, 4GB RAM, and 40GB storage, along with Docker Engine and Docker Compose." [Source: OneUptime blog, Harbor Vulnerability Scanning guide](https://oneuptime.com/blog/post/2026-02-08-how-to-run-harbor-container-registry-with-vulnerability-scanning/view)

A [December 2025 production readiness guide from VMware](https://blogs.vmware.com/cloud-foundation/2025/12/02/making-harbor-production-ready-essential-considerations-for-deployment/) emphasizes that production deployments require persistent external storage (NFS, S3-compatible), external PostgreSQL, and external Redis — substantially increasing operational surface area beyond the minimal Docker Compose install.

### 2.2 Vulnerability Scanning

"Harbor 2.8+ includes Trivy as the built-in vulnerability scanner." [Source: OneUptime blog, Harbor guide](https://oneuptime.com/blog/post/2026-02-08-how-to-run-harbor-container-registry-with-vulnerability-scanning/view) Trivy is installed at deployment time via the `--with-trivy` flag. Harbor can "automatically scan images on push and allows setting vulnerability severity thresholds (e.g., Critical or High)" to gate deployments. [Source: Aqua Security, harbor-scanner-trivy GitHub](https://github.com/aquasecurity/harbor-scanner-trivy)

A December 2025 CNCF blog confirmed Harbor's enterprise trajectory: "Harbor: Enterprise-grade container registry for modern private cloud." [Source: CNCF blog, December 2025](https://www.cncf.io/blog/2025/12/08/harbor-enterprise-grade-container-registry-for-modern-private-cloud/)

### 2.3 Garbage Collection and Storage Management

Harbor's garbage collection removes "dangling manifests and unreferenced blobs." During a garbage collection run, "Harbor goes into read-only mode and all modifications to the registry are prohibited." Scheduling options include Hourly, Daily, Weekly, or Custom cron. [Source: Harbor Docs, Garbage Collection](https://goharbor.io/docs/1.10/administration/garbage-collection/)

Harbor's 2025 "Satellite" feature enables bidirectional sync from a central registry to local edge registries, relevant for ISVs deploying CI artifacts to distributed customer sites. [Source: VMware Cloud Foundation blog, Harbor Production Ready](https://blogs.vmware.com/cloud-foundation/2025/12/02/making-harbor-production-ready-essential-considerations-for-deployment/)

### 2.4 Access Control

Harbor implements role-based access control (RBAC) with project-level isolation. "All operations to repositories are tracked through logs, providing audit capabilities for compliance purposes." [Source: Medium, Harbor on-premises guide by Asim Mirza](https://medium.com/@mughal.asim/harbor-as-your-on-premises-container-registry-for-cloud-native-deployments-341f8da6cdfc)

A [January 2026 VMware supply chain security post](https://blogs.vmware.com/cloud-foundation/2026/01/30/securing-your-software-supply-chain-with-harbor/) describes Harbor's proxy cache capability for upstream public registries, allowing teams to "use Harbor as a Proxy Cache for Cloud-Based Registries" — relevant for air-gapped scenarios where approved public images need controlled ingress. [Source: VMware blog, Harbor as Proxy Cache](https://blogs.vmware.com/cloud-foundation/2025/12/16/using-harbor-as-a-proxy-cache-for-cloud-based-registries/)

---

## 3. Artifact Management: Nexus Repository

### 3.1 Storage and Retention Policies

Sonatype Nexus Repository Manager 3 (NXRM3) supports hosted, proxy, and group repository types across formats including Maven, npm, PyPI, Docker, Helm, and NuGet. "Cleanup Policies and Tasks are not configured by default, and administrators should define the policies that best suit their development lifecycle." [Source: Sonatype Docs, Cleanup Policies](https://help.sonatype.com/en/cleanup-policies.html)

Cleanup criteria include component age (based on upload/update time for hosted repositories, first-download time for proxy repositories) and last-downloaded time. Retention policies require active configuration and scheduled task creation — they are not zero-touch. [Source: Sonatype Docs, Cleanup Policies](https://help.sonatype.com/en/cleanup-policies.html)

### 3.2 Proxy Caching for Dependency Management

A proxy repository in Nexus "functions as a substitute access point and managed cache for remote repositories, allowing organizations to centrally manage the cache." [Source: Sonatype Docs, Repository Manager Concepts](https://help.sonatype.com/en/repository-manager-concepts.html) In on-premises environments, this allows development teams to route all upstream dependency fetches through a controlled local cache, enabling dependency auditing and bandwidth reduction.

### 3.3 Air-Gapped Configuration

"Running NXRM3 inside an air-gapped network is possible, with administrators able to configure all NXRM originating outbound connections to use an HTTP proxy server which acts as a gate to other networks." [Source: Sonatype Support, Air-Gapped Considerations](https://support.sonatype.com/hc/en-us/articles/360049884673-Considerations-For-NXRM-3-Inside-Air-Gapped-Restricted-Firewalled-and-DMZ-Networks)

JFrog Artifactory describes a common two-instance air-gap architecture: "at least two Artifactory instances — one on the DMZ and another on the internal network — where the external instance downloads dependencies and exports them to an external device, then the internal instance imports them." [Source: JFrog blog, Using Artifactory with an Air Gap](https://jfrog.com/blog/using-artifactory-with-an-air-gap/)

### 3.4 Artifactory vs. Nexus Retention

These two tools take divergent approaches to cleanup. Nexus provides built-in cleanup policy UI. "Artifactory has no built-in retention mechanisms; instead, it relies on its query language (AQL) to identify artifacts and CLI or REST to delete them. Artifactory has the most advanced way of cleaning, though you need to have a standalone service running to do it." [Source: Eficode blog, Artifactory vs Nexus vs ProGet](https://www.eficode.com/blog/artifactory-nexus-proget)

---

## 4. Build Infrastructure

### 4.1 Dedicated Build Agents

On-premises build agents must be provisioned, operating system-hardened, and kept in sync with dependency toolchains (JDK versions, Node.js, Python interpreters, Docker). Agent fleet management typically uses Ansible, Puppet, or Chef for configuration consistency. [Source: Aziro comparison blog](https://www.aziro.com/en/blog/jenkins-vs-gitlab-vs-circleci-which-ci-cd-tool-is-right-for-you)

### 4.2 GPU-Enabled Build Agents for AI Workloads

For ISVs building AI-driven SaaS, testing ML models or building GPU-accelerated container images requires physical GPU hardware or virtualized GPU access (NVIDIA vGPU). Orchestrating GPU build agents on self-hosted Kubernetes requires the [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html), which manages GPU drivers, device plugins, and monitoring containers within Kubernetes.

[UNVERIFIED] Industry anecdote from a 2025 Northflank deployment guide states that "78% of teams struggle with GPU compatibility during CI/CD." This statistic is widely cited but the underlying survey methodology was not independently verifiable from the primary source. It is directionally plausible given the complexity of GPU driver pinning across kernel versions.

ClearML is one platform that offers an "Infrastructure Control Plane" allowing organizations to "connect and manage GPU clusters — whether on-premises, in the cloud, or both — ensuring high performance and cost optimization." [Source: ClearML AI Infrastructure Platform](https://clear.ml/)

### 4.3 Build Cache Management

Docker BuildKit provides the primary build caching mechanism. "An external cache becomes almost essential in CI/CD build environments where such environments usually have little-to-no persistence between runs." [Source: SparkFabrik blog, Docker BuildKit Deep Dive](https://tech.sparkfabrik.com/en/blog/docker-cache-deep-dive/)

Quantified impact: "By implementing a smart Docker layer caching strategy, you can slash your container build times, often by 70% or more." [Source: Netdata Academy, Docker Layer Caching](https://www.netdata.cloud/academy/docker-layer-caching/)

More dramatic results are possible: one documented case shows layer caching "reducing build time of the Grafana Docker image from seven minutes to just twenty seconds." [Source: Harness blog, CI Docker Layer Caching](https://www.harness.io/blog/ci-docker-layer-caching/)

For on-premises setups, BuildKit cache can be pushed to a local Harbor or Nexus OCI endpoint using `--cache-to type=registry` and `--cache-from type=registry`. [Source: Docker Docs, Cache Storage Backends](https://docs.docker.com/build/cache/backends/)

For language-level caches, "language caches like ~/.npm, ~/.cache/pip, Maven/Gradle caches keyed on lockfiles are the highest-leverage accelerator for CI/CD pipeline performance." [Source: Datadog blog, Cache Purging in CI/CD](https://www.datadoghq.com/blog/cache-purge-ci-cd/)

---

## 5. Pipeline Authoring

### 5.1 Multi-Stage Pipelines and Parallelization

"Structure your pipeline to run the fastest, most critical tests first — quick unit and static analysis checks immediately — then execute longer-running integration or end-to-end tests in parallel across multiple agents or containers to dramatically reduce total execution time." [Source: Spacelift blog, CI/CD Best Practices](https://spacelift.io/blog/ci-cd-best-practices/)

Both Jenkins (via `parallel` directive in Declarative Pipeline) and GitLab CI (via `parallel` keyword and DAG pipeline syntax) support job-level parallelism. Woodpecker CI supports parallel steps within a pipeline stage natively.

### 5.2 Shared Libraries and Reusable Components

Jenkins supports [Shared Libraries](https://www.jenkins.io/doc/book/pipeline/shared-libraries/) stored in separate SCM repositories, loaded at pipeline execution time. GitLab CI supports `include` directives referencing external YAML files or templates, enabling central maintenance of reusable job definitions.

"Keep pipelines modular: split unit tests, integration tests, security scans, and performance tests into distinct stages with clear pass/fail criteria and time budgets." [Source: DiNeuron blog, CI/CD Best Practices 2025](https://dineuron.com/cicd-pipeline-best-practices-for-modern-development-2025-complete-implementation-guide/)

### 5.3 Environment Promotion

"A modern CI/CD pipeline connects source control triggers to isolated runners, builds and tests code, generates signed artifacts (container images, SBOMs, test reports), and promotes versions through environments with explicit gates before deployment." [Source: Gravitee blog, Scalable CI/CD Pipelines](https://www.gravitee.io/blog/reliable-ci-cd-pipelines-faster-software-releases/)

On-premises environment promotion requires teams to build their own gate logic (approval steps, policy checks) or integrate external tools such as OPA (Open Policy Agent) or in-cluster admission controllers. Managed services offer native approval environments with audit logs.

---

## 6. Testing Infrastructure

### 6.1 Self-Hosted Test Environments and Database Fixtures

CircleCI's ML testing guide describes a layered testing approach where "Pytest's builtin fixture is a function executed before the test function, with fixture definitions placed in a conftest.py script." [Source: Made With ML / Anyscale, Testing ML Systems](https://madewithml.com/courses/mlops/testing/)

On-premises, teams must self-manage ephemeral test databases. Common approaches include Docker Compose for spinning up PostgreSQL/Redis fixtures within the pipeline job, or a shared staging cluster with namespace isolation per pipeline run. Fixture teardown must be explicit to avoid state leakage between runs.

### 6.2 GPU Access for ML Model Testing

"Self-hosted dedicated servers make automated machine learning workflows more stable and predictable because the same resources are always available for training, testing, and deployment." [Source: Medium, Self-Hosted Automated ML Pipeline](https://medium.com/@emilyharbord2/build-a-self-hosted-automated-ml-pipeline-on-a-dedicated-server-8d22fe4a8b4b/)

A common self-hosted ML testing stack documented in 2025 community guides: MinIO (S3-compatible object storage for datasets and model artifacts) + PostgreSQL (metadata) + MLflow (experiment tracking and model registry) + Prometheus/Grafana (infrastructure observability). [Source: Medium, Self-Hosted Automated ML Pipeline](https://medium.com/@emilyharbord2/build-a-self-hosted-automated-ml-pipeline-on-a-dedicated-server-8d22fe4a8b4b/)

GPU sharing across concurrent test jobs requires NVIDIA's Multi-Instance GPU (MIG) or time-slicing configuration, adding another operational layer.

---

## 7. Air-Gapped Considerations

### 7.1 Dependency Mirroring

"In an air-gapped CI/CD pipeline, a local mirror of required dependencies must be maintained using tools like Nexus Repository, Artifactory, or an internal package registry." [Source: Improwised Tech blog, CI/CD in Air-Gapped Environments](https://www.improwised.com/blog/ci-cd-in-air-gapped-environments/)

"In a standard setup, build tools automatically download dependencies from public repositories like Maven Central or npmjs.com, but teams must become proficient in processes for the manual import and vetting of new dependencies." [Source: ones.com blog, CI/CD on Air-Gapped Platforms](https://ones.com/blog/7-key-considerations-for-implementing-cicd-on-air-gapped-platforms-without-cloud-access/)

### 7.2 Offline Package Repositories and Image Pre-Pulling

"In a fully-disconnected air-gap, the general process is that you copy the necessary data from your outside (online) server and manually transfer it to your inside (offline) server via physical media." [Source: ones.com blog, Air-Gapped Platform Setup](https://ones.com/blog/how-to-set-up-an-air-gapped-platform-with-cicd-integration/)

"For containerized deployments, a self-hosted container registry is required, and approved container images must be manually imported and periodically updated from external sources." [Source: Improwised Tech blog, CI/CD in Air-Gapped Environments](https://www.improwised.com/blog/ci-cd-in-air-gapped-environments/)

GitLab provides dedicated [offline deployment documentation](https://docs.gitlab.com/user/application_security/offline_deployments/) for its security scanning features (SAST, DAST, dependency scanning), including instructions for pre-loading analyzer Docker images.

### 7.3 Security Scanning in Air-Gapped Environments

"Since scanning tools like SAST and SCA cannot access cloud services for updated vulnerability databases, the entire scanning engine and its data must be updated manually." [Source: Improwised Tech blog](https://www.improwised.com/blog/ci-cd-in-air-gapped-environments/) This creates a procedural requirement: a scheduled, human-supervised process to import updated CVE databases from an external transfer point into the internal Trivy/Grype database.

Tools supporting air-gapped security scanning as of 2025 include: self-hosted GitLab, Woodpecker CI, SonarQube Community Edition (with offline rule packs), and Trivy with a local offline database. [Source: Programming Insider, CI/CD Resilience](https://programminginsider.com/ci-cd-resilience-combining-ai-gitops-offline-tools-for-robust-pipelines/)

---

## 8. Comparison: Managed CI/CD Services vs. On-Premises

When an ISV moves from on-premises to a managed CI/CD service, the following operational burdens disappear:

| Responsibility | On-Premises | Managed Service |
|---|---|---|
| CI server OS patching | Team | Vendor |
| Plugin/extension upgrades | Team | Vendor (or not applicable) |
| Runner fleet provisioning | Team | Vendor (or pay-per-minute autoscale) |
| Container registry storage management | Team | Vendor (pay-per-GB) |
| Vulnerability scanner database updates | Team | Vendor |
| TLS certificate management | Team | Vendor |
| High-availability failover | Team | Vendor SLA |
| Compliance audit logs | Team must build | Vendor-native |

"GitHub Actions eliminates much of the infrastructure management required by self-hosted CI/CD systems. GitHub handles core infrastructure, updates, and scaling, freeing your team to focus on pipeline development rather than platform maintenance." [Source: Buildkite blog, GitHub Actions vs Jenkins](https://buildkite.com/resources/ci-cd-perspectives/github-actions-vs-jenkins-which-one-s-right-for-your-team/)

"Microsoft-hosted agents in Azure DevOps are maintained by Microsoft, offering preconfigured, cloud-based execution environments that support Windows, Linux, and macOS." [Source: TechTarget, Azure DevOps vs GitHub CI/CD](https://www.techtarget.com/searchcloudcomputing/tip/Compare-Azure-DevOps-vs-GitHub-for-CI-CD-pipelines/)

"Over 4 million workflows were running daily on GitHub Actions in 2025." [Source: MDPI, Practical Comparison Azure DevOps and GitHub](https://www.mdpi.com/1999-5903/17/4/153/)

**Cost dynamics of managed services:** "Starting March 2026, GitHub is introducing a $0.002 per minute platform charge for self-hosted runner usage in private repositories." [Source: Northflank blog, GitHub Pricing Changes](https://northflank.com/blog/github-pricing-change-self-hosted-alternatives-github-actions/) This erodes the historical cost advantage of self-hosted runners for private-repo ISV workloads.

**Managed registry comparison:** Cloud-hosted registries (ECR, ACR, Artifact Registry) offer "minimal operational overhead, automatic updates, and pay-per-use pricing" with native IAM integration. Harbor on-premises offers "complete control, works in air-gapped environments, and avoids cloud egress fees but requires operational management." [Source: Shipyard blog, Container Registries 2026](https://shipyard.build/blog/container-registries/)

---

## 9. Comparison Table

| Capability | On-Premises | Managed Kubernetes (EKS/AKS/GKE) | Cloud-Native |
|---|---|---|---|
| **CI Server** | Difficulty: 4/5 | Difficulty: 3/5 | Difficulty: 1/5 |
| | Jenkins/GitLab Self-Managed/Woodpecker; full OS + plugin lifecycle ownership | GitLab Runner on cluster; Kubernetes autoscaling; cluster control plane managed by provider | GitHub Actions / Azure DevOps / Cloud Build; zero infrastructure ownership |
| | Ansible/Puppet for config; LDAP integration | Helm charts; GitLab Operator or Woodpecker on K8s | Native SaaS; YAML pipelines only |
| | Est. FTE: 0.75–1.0 (dedicated CI admin) | Est. FTE: 0.5 (K8s operator familiar) | Est. FTE: 0.1 (pipeline authoring only) |
| **Container Registry** | Difficulty: 3/5 | Difficulty: 2/5 | Difficulty: 1/5 |
| | Harbor with external PostgreSQL, Redis, S3 storage; garbage collection scheduling; Trivy database updates | Harbor on cluster (Helm); persistent volume management; provider-managed control plane | ECR / ACR / Artifact Registry; pay-per-GB; native IAM |
| | Retention policies; TLS certificates; HA failover | Horizontal pod autoscaler; persistent volume claims | Automatic replication; lifecycle policies |
| | Est. FTE: 0.25–0.5 | Est. FTE: 0.15 | Est. FTE: 0.05 |
| **Artifact Management** | Difficulty: 3/5 | Difficulty: 2/5 | Difficulty: 1/5 |
| | Nexus/Artifactory; cleanup policy authoring; proxy repo caching; license compliance scanning | Nexus/Artifactory on K8s; PVC for data; same policy authoring burden | Cloud Artifact Registry / GitHub Packages; vendor-managed storage; limited format support |
| | Air-gap two-instance DMZ topology | Same topology possible within K8s network policies | Not air-gap compatible |
| | Est. FTE: 0.25–0.5 | Est. FTE: 0.20 | Est. FTE: 0.05 |
| **Build Agent Fleet** | Difficulty: 4/5 | Difficulty: 3/5 | Difficulty: 1/5 |
| | Physical/VM agents; manual GPU provisioning; NVIDIA GPU Operator; cache volumes; toolchain pinning | Dynamic pod-based agents; NVIDIA GPU Operator on cluster; shared cache via PVC or registry | Cloud runners (standard CPU); GPU runners limited/expensive; no local cache persistence |
| | Ansible/Chef for agent config; NVMe local caches | KEDA autoscaling; resource quotas; node taints for GPU | Fixed runner sizes; no custom hardware |
| | Est. FTE: 0.5–0.75 | Est. FTE: 0.30 | Est. FTE: 0.0 (managed) |
| **Testing Infrastructure** | Difficulty: 4/5 | Difficulty: 3/5 | Difficulty: 2/5 |
| | Self-managed ephemeral DBs (Docker Compose); GPU fixture access; MLflow + MinIO self-hosted | Namespace-isolated test environments on cluster; shared GPU node pool | Cloud managed DBs as test fixtures; no GPU for standard runners; managed ML services |
| | Full fixture lifecycle ownership; CVE scan data manual updates | K8s Jobs for fixture teardown; PVC snapshots | Vendor SLA on test infra availability |
| | Est. FTE: 0.25–0.5 | Est. FTE: 0.15–0.25 | Est. FTE: 0.10 |

**Total Estimated FTE (on-premises full stack):** 2.0–3.25 FTE dedicated DevOps/platform engineers, excluding on-call burden. On-call adds approximately 0.25–0.5 FTE equivalent in annual interrupt-driven hours for a team of 20–50 developers.

**FTE Assumptions:** Estimates assume a mid-size ISV team (20–50 developers), a single production-grade environment per deployment model, and engineers with intermediate Kubernetes/Linux experience. Air-gap requirements add approximately 0.5 FTE for dependency mirroring and vulnerability database management. GPU-enabled build requirements add approximately 0.25 FTE for driver lifecycle and scheduling configuration.

---

## Key Takeaways

- **On-premises CI/CD is a full platform product.** Operating Jenkins, Harbor, Nexus, build agents, and test infrastructure on-premises requires 2.0–3.5 dedicated FTE and continuous security patching — Jenkins alone issued nine public security advisories in 2025. ISVs must treat their internal CI/CD stack as a maintained product, not a one-time installation.

- **Air-gapped requirements dominate complexity.** The single largest operational multiplier in on-premises CI/CD is the air-gapped or network-restricted use case. Dependency mirroring, two-instance Nexus/Artifactory topologies, manual CVE database imports, and image pre-pulling all require human-supervised recurring processes that managed services eliminate by design.

- **GPU-enabled build and test infrastructure is the emerging differentiator.** ISVs building AI-driven SaaS products require GPU access in their CI/CD pipelines for model testing and GPU container image validation. This capability is unavailable or prohibitively expensive on standard managed runner tiers, making on-premises or Managed Kubernetes the only viable paths for AI workload CI/CD today.

- **Managed Kubernetes (EKS/AKS/GKE) is the pragmatic middle path.** Running GitLab Runner or Woodpecker CI agents on a managed Kubernetes cluster preserves data residency and custom hardware access while offloading Kubernetes control-plane operations to the provider. Estimated FTE savings versus fully on-premises range from 0.5–1.0 FTE annually for a mid-size ISV.

- **Build caching is the highest-ROI investment across all deployment models.** Docker layer caching via BuildKit reduces container build times by 70% or more; in documented cases, build times dropped from seven minutes to twenty seconds. Investing in a shared cache registry (Harbor or ECR as a BuildKit cache backend) delivers returns at every deployment model and should precede infrastructure scaling investments.

---

## Sources

- [Self-Hosted GitLab: Setting Up a Private CI/CD Server — Cycle.io](https://cycle.io/learn/self-hosted-gitlab)
- [Jenkins vs. GitLab vs. CircleCI — Aziro blog](https://www.aziro.com/en/blog/jenkins-vs-gitlab-vs-circleci-which-ci-cd-tool-is-right-for-you)
- [Jenkins Security Advisories 2025 — Jenkins.io](https://www.jenkins.io/security/advisories/)
- [Jenkins Security Advisory 2025-12-10](https://www.jenkins.io/security/advisory/2025-12-10/)
- [Jenkins Security Advisory 2025-09-03](https://www.jenkins.io/security/advisory/2025-09-03/)
- [Jenkins Security Advisory 2025-04-02](https://www.jenkins.io/security/advisory/2025-04-02/)
- [Jenkins Security Advisory 2025-01-22](https://www.jenkins.io/security/advisory/2025-01-22/)
- [Multiple Jenkins Plugin Vulnerabilities Expose Sensitive Data — CyberPress](https://cyberpress.org/jenkins-plugin-vulnerabilities/)
- [GitLab Runner Fleet Scaling — GitLab Docs](https://docs.gitlab.com/runner/fleet_scaling/)
- [GitLab Runner Autoscaling — GitLab Docs](https://docs.gitlab.com/runner/runner_autoscale/)
- [Optimizing GitLab CI Runners on Kubernetes — ChairNerd/SeatGeek](https://chairnerd.seatgeek.com/ci-runner-optimizations/)
- [Woodpecker CI — Official Site](https://woodpecker-ci.org/)
- [woodpecker-ci/woodpecker — GitHub](https://github.com/woodpecker-ci/woodpecker)
- [Breaking Up with GitHub Actions: Woodpecker CI — Kalvad blog](https://blog.kalvad.com/breaking-up-with-github-actions-our-love-story-with-woodpecker-ci/)
- [Harbor as On-Premises Container Registry — Asim Mirza, Medium](https://medium.com/@mughal.asim/harbor-as-your-on-premises-container-registry-for-cloud-native-deployments-341f8da6cdfc)
- [Harbor Enterprise-Grade Container Registry — CNCF blog, December 2025](https://www.cncf.io/blog/2025/12/08/harbor-enterprise-grade-container-registry-for-modern-private-cloud/)
- [How to Run Harbor with Vulnerability Scanning — OneUptime blog](https://oneuptime.com/blog/post/2026-02-08-how-to-run-harbor-container-registry-with-vulnerability-scanning/view)
- [harbor-scanner-trivy — Aqua Security, GitHub](https://github.com/aquasecurity/harbor-scanner-trivy)
- [Making Harbor Production-Ready — VMware Cloud Foundation blog, December 2025](https://blogs.vmware.com/cloud-foundation/2025/12/02/making-harbor-production-ready-essential-considerations-for-deployment/)
- [Securing Your Software Supply Chain with Harbor — VMware blog, January 2026](https://blogs.vmware.com/cloud-foundation/2026/01/30/securing-your-software-supply-chain-with-harbor/)
- [Using Harbor as a Proxy Cache — VMware blog, December 2025](https://blogs.vmware.com/cloud-foundation/2025/12/16/using-harbor-as-a-proxy-cache-for-cloud-based-registries/)
- [Harbor Garbage Collection — Harbor Docs](https://goharbor.io/docs/1.10/administration/garbage-collection/)
- [Sonatype Nexus Repository Cleanup Policies — Sonatype Docs](https://help.sonatype.com/en/cleanup-policies.html)
- [Repository Manager Concepts — Sonatype Docs](https://help.sonatype.com/en/repository-manager-concepts.html)
- [Considerations For NXRM3 in Air-Gapped Networks — Sonatype Support](https://support.sonatype.com/hc/en-us/articles/360049884673-Considerations-For-NXRM-3-Inside-Air-Gapped-Restricted-Firewalled-and-DMZ-Networks)
- [Artifactory vs Nexus vs ProGet — Eficode blog](https://www.eficode.com/blog/artifactory-nexus-proget)
- [No Internet? No Problem. Using Artifactory with an Air Gap — JFrog blog](https://jfrog.com/blog/using-artifactory-with-an-air-gap/)
- [Docker BuildKit Deep Dive — SparkFabrik Tech blog](https://tech.sparkfabrik.com/en/blog/docker-cache-deep-dive/)
- [Docker Layer Caching in CI Pipelines Cut Build Times by 70% — Netdata Academy](https://www.netdata.cloud/academy/docker-layer-caching/)
- [CI Docker Layer Caching — Harness blog](https://www.harness.io/blog/ci-docker-layer-caching/)
- [Cache Storage Backends — Docker Docs](https://docs.docker.com/build/cache/backends/)
- [Patterns for Safe and Efficient Cache Purging in CI/CD — Datadog blog](https://www.datadoghq.com/blog/cache-purge-ci-cd/)
- [ClearML AI Infrastructure Platform](https://clear.ml/)
- [Build a Self-Hosted Automated ML Pipeline — Emily Harbord, Medium, January 2026](https://medium.com/@emilyharbord2/build-a-self-hosted-automated-ml-pipeline-on-a-dedicated-server-8d22fe4a8b4b/)
- [Testing Machine Learning Systems — Made With ML / Anyscale](https://madewithml.com/courses/mlops/testing/)
- [CI/CD Best Practices — Spacelift blog](https://spacelift.io/blog/ci-cd-best-practices/)
- [CI/CD Pipeline Best Practices 2025 — DiNeuron blog](https://dineuron.com/cicd-pipeline-best-practices-for-modern-development-2025-complete-implementation-guide/)
- [Scalable CI/CD Pipelines Best Practices — Gravitee blog](https://www.gravitee.io/blog/reliable-ci-cd-pipelines-faster-software-releases/)
- [CI/CD in Air-Gapped Environments — Improwised Tech blog](https://www.improwised.com/blog/ci-cd-in-air-gapped-environments/)
- [7 Key Considerations for CI/CD on Air-Gapped Platforms — ones.com blog](https://ones.com/blog/7-key-considerations-for-implementing-cicd-on-air-gapped-platforms-without-cloud-access/)
- [Air-Gapped Platform with CI/CD Integration — ones.com blog](https://ones.com/blog/how-to-set-up-an-air-gapped-platform-with-cicd-integration/)
- [Offline Deployments — GitLab Docs](https://docs.gitlab.com/user/application_security/offline_deployments/)
- [CI/CD Resilience: AI, GitOps & Offline Tools — Programming Insider](https://programminginsider.com/ci-cd-resilience-combining-ai-gitops-offline-tools-for-robust-pipelines/)
- [GitHub Actions vs. Jenkins — Buildkite blog](https://buildkite.com/resources/ci-cd-perspectives/github-actions-vs-jenkins-which-one-s-right-for-your-team/)
- [Compare Azure DevOps vs. GitHub for CI/CD — TechTarget](https://www.techtarget.com/searchcloudcomputing/tip/Compare-Azure-DevOps-vs-GitHub-for-CI-CD-pipelines/)
- [Practical Comparison Azure DevOps and GitHub — MDPI, 2025](https://www.mdpi.com/1999-5903/17/4/153/)
- [GitHub Pricing Changes, Self-Hosted Alternatives — Northflank blog](https://northflank.com/blog/github-pricing-change-self-hosted-alternatives-github-actions/)
- [Choosing a Container Registry 2026: Docker Hub vs ECR vs Harbor — Shipyard blog](https://shipyard.build/blog/container-registries/)
- [Comparing Container Registries: ECR vs ACR vs GCR — InfoBytes.guru](https://infobytes.guru/articles/container-registry-comparison.html)
- [AWS Remote Build Cache in ECR — InfoQ, November 2025](https://www.infoq.com/news/2025/11/aws-remote-build-cache-docker/)
