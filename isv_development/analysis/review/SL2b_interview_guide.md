# SL2b: Consolidated Interview Validation Guide

**Synthesis Date:** 2026-02-19
**Scope:** All Medium and Low confidence ratings across Phases 1, 2, and 3
**Input Sources:** SL1a, SL1b, SL1c, RP1a, RP1b, RP2a, RP2b, RP3a, RP3b, RP3c, RP4a, CC5
**Output Type:** Interview guide organized by role, prioritized by confidence impact

---

## Executive Summary

Across the three-phase review of 38 MECE subsegments, approximately 50 interview questions were generated by 13 review agents to validate Medium-confidence ratings and resolve borderline findings. This guide consolidates, deduplicates, and prioritizes those questions into 35 unique questions organized by seven interviewee roles, with a "Top 10 Must-Ask" subset that would resolve the highest-impact confidence gaps -- collectively touching 20+ ratings across all four planes. The interview program targets ISV practitioners who have completed at least one cloud-to-on-premises migration and currently operate 10+ customer deployments; a minimum viable interview set of 3 roles (VP Engineering, Platform Engineer, Data Engineer) across 2 interviews each would cover the Top 10 questions in approximately 6 hours of total interview time.

---

## Interview Logistics

### Target Interviewee Profile

All questions assume the interviewee works at a mid-size ISV (50-500 employees) that has:
- Shipped a SaaS product on cloud-native infrastructure (AWS/Azure/GCP)
- Completed at least one on-premises deployment to a customer's Kubernetes environment
- Currently supports 10+ on-premises customer deployments in production

### Recommended Format

| Parameter | Recommendation |
|---|---|
| Format | Semi-structured (scripted core questions with follow-up probes) |
| Recording | Audio + transcript with interviewee consent; no video required |
| Duration per session | 60 minutes maximum; 45 minutes preferred |
| Warm-up | 5 minutes: role, company, on-premises deployment count, timeline |
| Core questions | 30-35 minutes: 5-7 questions from the role-specific list below |
| Open-ended close | 5-10 minutes: "What did we not ask about that surprised your team?" |
| Debrief | Interviewer records confidence adjustments within 24 hours |

### Sessions by Role

| Role | Number of Sessions | Estimated Duration | Priority |
|---|:---:|---|:---:|
| VP Engineering / CTO | 2 sessions | 60 min each | Critical |
| Platform Engineer / DevOps Lead | 2 sessions | 60 min each | Critical |
| Data Platform Engineer / DBA | 1-2 sessions | 45-60 min each | Critical |
| SRE / AI Platform Engineer | 1-2 sessions | 45-60 min each | High |
| Principal / Staff Engineer | 1 session | 60 min | High |
| Compliance / Security Lead | 1 session | 45 min | Medium |
| Backend Engineer | 1 session | 45 min | Medium |
| **Total** | **9-11 sessions** | **~8-10 hours** | |

---

## Top 10 Must-Ask Questions

These questions are selected for maximum confidence impact: each would resolve uncertainty across 3 or more ratings if answered empirically. They are listed in priority order.

### Q1. Aggregate Phase 1 Effort Validation
**Target:** VP Engineering / CTO
**Question:** "For your initial on-premises refactoring -- the one-time engineering investment to make your cloud-native SaaS portable to customer-hosted Kubernetes -- how many total person-months did the project consume, and over how many calendar months? How large was the team?"
**Validates:** P1 aggregate effort estimate (72-149 person-months), parallelization factor (0.55-0.67x), P1 Phase 1 averages (RD 4.4, TE 4.0)
**Source:** SL1a Section 4 (from RP1e)
**Priority Tier:** Tier 1 -- changes 5+ ratings if answered

### Q2. Per-Customer Deployment Time (P1 Control Plane)
**Target:** Platform Engineer / DevOps Lead
**Question:** "For each new on-premises customer deployment, how many engineer-weeks does your team spend on K8s cluster configuration specific to that customer's server hardware and virtualization layer, excluding work that's standardized across all customers?"
**Validates:** CP-01 Phase 2 RD=4/TE=4, aggregate Phase 2 P1 effort (6-14 person-weeks), air-gapped multiplier
**Source:** RP1a (CP-01 Phase 2 Medium confidence)
**Priority Tier:** Tier 1 -- changes 3+ ratings if answered

### Q3. Network Configuration Per Customer
**Target:** Platform Engineer / SRE Lead
**Question:** "When deploying to a new on-premises customer site, how much time is consumed specifically by network configuration -- firewall rules, proxy bypass, DNS integration, and CNI configuration -- versus all other deployment tasks?"
**Validates:** CP-02 Phase 2 RD=4/TE=4 (most variable P1 subsegment per customer), P1 Phase 2 aggregate
**Source:** SL1b Section 5.2 (from RP1d); RP1a (CP-02 Phase 2 Medium confidence)
**Priority Tier:** Tier 1 -- changes 3+ ratings if answered

### Q4. Phase 3 FTE Headcount vs. Projection
**Target:** VP Engineering / CTO
**Question:** "How many FTEs are currently dedicated to on-premises customer support operations across all engineering functions? How does this compare to your original staffing model, and what was the biggest surprise in where FTE accumulated?"
**Validates:** Phase 3 grand total (~51-95 FTE), P2 systematic divergence (+0.9 TE-RD gap), all Phase 3 FTE ranges
**Source:** SL1c Section 6 (VP Engineering Q1); CC5 talent acquisition gap
**Priority Tier:** Tier 1 -- changes 5+ ratings if answered

### Q5. Data Service Per-Customer Deployment Time
**Target:** Data Platform Engineer / DBA
**Question:** "When you deploy your data stack to a new customer environment, how much of the work is parameter-file changes versus active investigation of the customer's storage controller, memory profile, or network topology?"
**Validates:** DS1/DS6/DS8/DS9 Phase 2 TE=2 ratings, hidden Phase 1 template dependency, P3 Phase 2 aggregate (2-4 vs. 3-6 person-weeks)
**Source:** SL1b Section 5.3 (from RP3c)
**Priority Tier:** Tier 1 -- changes 4+ ratings if answered

### Q6. AI/ML Orchestration Ongoing Difficulty
**Target:** Principal / Staff Engineer
**Question:** "When you upgrade LangGraph or an MCP server version in a customer's on-premises deployment, how many other software components in the AI orchestration stack (Temporal, Langfuse, LiteLLM, guardrails) require coordinated testing or updates in the same maintenance window? Does this coordination work fall on the application team or the deployment team?"
**Validates:** AL09 Phase 3 RD=3 (proposed adjustment to RD=4), CP-07 scope boundary, P2 Phase 3 systematic divergence
**Source:** RP2b (AL09 Phase 3 Medium confidence)
**Priority Tier:** Tier 1 -- changes 3+ ratings if answered

### Q7. IAM FTE at Scale
**Target:** VP Engineering / Platform Engineering Director
**Question:** "How many FTE does your team spend annually on IAM-specific work -- Keycloak version upgrades, per-customer federation maintenance, RBAC policy updates, and identity incident response -- across your on-premises customer base? How much of that is shared work versus per-customer?"
**Validates:** CP-03 Phase 3 TE=4 (proposed adjustment to TE=5), CP-03 Phase 2 TE=3, security cluster FTE deduplication
**Source:** RP1a (CP-03 Phase 3 Medium confidence)
**Priority Tier:** Tier 1 -- changes 3+ ratings if answered

### Q8. Time-to-Hire for Platform Engineers
**Target:** VP Engineering / CTO
**Question:** "What is your average time-to-hire for platform engineers with Kubernetes operations experience? Does it exceed 5 months? How has this affected your ability to staff the on-premises support function?"
**Validates:** CC5 talent acquisition gap, Phase 3 FTE executability, P1 Phase 3 staffing risk
**Source:** SL1c Section 6 (CC5 structural gap)
**Priority Tier:** Tier 2 -- changes 1-2 ratings but validates all Phase 3 FTE estimates

### Q9. Air-Gapped Deployment Multiplier
**Target:** Platform Engineer / DevOps Lead
**Question:** "What percentage of your on-premises customers require air-gapped deployment, and what is the total engineering time consumed by artifact bundle preparation and delivery for an air-gapped customer versus a proxy-accessible customer?"
**Validates:** CP-06 Phase 2 TE=2 (annotation: TE=3 for air-gapped), P1 Phase 2 aggregate for air-gapped segment (9-18 person-weeks)
**Source:** SL1b Section 5.2 (from RP1d)
**Priority Tier:** Tier 2 -- changes 1-2 ratings, validates P1 Phase 2 customer segmentation

### Q10. Temporal and Dead-Letter Queue Ownership
**Target:** Principal / Staff Engineer or Backend Engineer
**Question:** "For your on-premises Temporal deployment, how frequently do Temporal version upgrades require changes to your workflow SDK code or configuration files? And who owns your dead-letter queue handling code -- platform engineers or backend engineers?"
**Validates:** AL05 Phase 3 RD=2 (proposed adjustment to RD=3), Temporal config drift burden, DLQ ownership boundary
**Source:** RP2a (AL05 Phase 3 Medium confidence)
**Priority Tier:** Tier 2 -- changes 1-2 ratings, validates P2 Phase 3 divergence

---

## Questions by Interviewee Role

### Role 1: VP Engineering / CTO (8 questions)

**VE-1. Aggregate Phase 1 Effort** [Top 10 Q1]
"For your initial on-premises refactoring -- the one-time engineering investment to make your cloud-native SaaS portable to customer-hosted Kubernetes -- how many total person-months did the project consume, and over how many calendar months? How large was the team?"
- Validates: Phase 1 aggregate (72-149 pm), parallelization factor
- Source: SL1a Section 4 (RP1e)
- Priority: Tier 1

**VE-2. Phase 3 FTE Headcount** [Top 10 Q4]
"How many FTEs are currently dedicated to on-premises customer support operations? How does this compare to the 51-95 FTE range projected here?"
- Validates: Phase 3 grand total, P2 divergence, all FTE ranges
- Source: SL1c Section 6
- Priority: Tier 1

**VE-3. Time-to-Hire** [Top 10 Q8]
"What is your average time-to-hire for platform engineers with K8s operations experience? Does it exceed 5 months?"
- Validates: CC5 talent acquisition gap, Phase 3 executability
- Source: SL1c Section 6 (CC5)
- Priority: Tier 2

**VE-4. IAM Familiarity**
"What is the ISV's current identity provider familiarity -- have they previously operated Keycloak or Dex in production? Did this prior experience materially reduce the Phase 1 IAM build timeline?"
- Validates: CP-03 Phase 1 TE=4 (M confidence), P1 lower-bound feasibility
- Source: SL1a Section 4 (RP1e)
- Priority: Tier 2

**VE-5. SOC Scope in Phase 1**
"Does the ISV plan to build 24/7 SOC capability in Phase 1, or is CP-10 scoped to runtime detection tooling plus incident runbooks?"
- Validates: CP-10 Phase 1 TE=4 (M confidence), P1 scope boundary
- Source: SL1a Section 4 (RP1e)
- Priority: Tier 2

**VE-6. Organizational Structure**
"Is the organization structured with separate cloud-native and on-premises support teams, or is there a shared team? What was the organizational change cost?"
- Validates: CC5 organizational change management gap, Phase 3 team structure
- Source: SL1c Section 6
- Priority: Tier 3

**VE-7. Concurrent K8s Versions**
"How many concurrent Kubernetes minor versions do you support across the customer base? Does the 3-5 concurrent version assumption in CP-07 match reality?"
- Validates: CP-07 Phase 3 RD=5/TE=5, N-scaling validation
- Source: SL1c Section 6
- Priority: Tier 3

**VE-8. IAM FTE at Scale** [Top 10 Q7]
"How many FTE does your team spend annually on IAM-specific work -- Keycloak version upgrades, per-customer federation maintenance, RBAC policy updates -- across your on-premises customer base? How much of that is shared work versus per-customer?"
- Validates: CP-03 Phase 3 TE=4 vs. TE=5, deduplication methodology
- Source: RP1a (CP-03 Phase 3)
- Priority: Tier 1

---

### Role 2: Platform Engineer / DevOps Lead (8 questions)

**PE-1. Per-Customer K8s Configuration** [Top 10 Q2]
"For each new on-premises customer deployment, how many engineer-weeks does your team spend on K8s cluster configuration specific to that customer's server hardware and virtualization layer, excluding work that's standardized across all customers?"
- Validates: CP-01 Phase 2 RD=4/TE=4 (M confidence)
- Source: RP1a (CP-01 Phase 2)
- Priority: Tier 1

**PE-2. Network Configuration Time** [Top 10 Q3]
"When deploying to a new on-premises customer site, how much time is consumed specifically by network configuration -- firewall rules, proxy bypass, DNS integration, and CNI configuration -- versus all other deployment tasks?"
- Validates: CP-02 Phase 2 RD=4/TE=4 (most variable P1 subsegment)
- Source: SL1b Section 5.2 (RP1d); RP1a (CP-02 Phase 2)
- Priority: Tier 1

**PE-3. PKI Integration Complexity**
"For customers with their own PKI infrastructure or HSMs, how many engineer-hours does the certificate authority trust chain integration and Vault-to-HSM seal configuration typically require per customer, and have you encountered customers where PKI integration became a deployment blocker?"
- Validates: CP-04 Phase 2 RD=3/TE=3 (M confidence), annotation for RD=4 in regulated industries
- Source: SL1b Section 5.2 (RP1d)
- Priority: Tier 2

**PE-4. Air-Gapped Deployment** [Top 10 Q9]
"What percentage of your on-premises customers require air-gapped deployment, and what is the total engineering time consumed by artifact bundle preparation and delivery for an air-gapped customer versus a proxy-accessible customer?"
- Validates: CP-06 Phase 2 TE=2 (annotation: TE=3 for air-gapped)
- Source: SL1b Section 5.2 (RP1d)
- Priority: Tier 2

**PE-5. Maintenance Window Negotiation**
"When establishing the first deployment and rollback procedures with a new on-premises customer, how much time is consumed negotiating maintenance windows, change management processes, and validating the first rollback drill -- and does this differ materially between regulated and commercial customers?"
- Validates: CP-07 Phase 2 RD=3/TE=3 (annotation: TE=4 for regulated)
- Source: SL1b Section 5.2 (RP1d)
- Priority: Tier 2

**PE-6. FedRAMP vs. SOC 2 Delta**
"When deploying for a customer under FedRAMP versus a customer under SOC 2 only, how does the compliance configuration engineering time differ per deployment -- and does your team treat FedRAMP as a distinct deployment track with separate per-customer cost accounting?"
- Validates: CP-09 Phase 2 RD=3/TE=3 (annotation: TE=4 for FedRAMP)
- Source: SL1b Section 5.2 (RP1d)
- Priority: Tier 2

**PE-7. Vault Seal/Unseal Procedure**
"In practice, how many distinct manual steps does a Vault upgrade across N customer environments require -- is the seal/unseal procedure during Kubernetes pod restarts automated via auto-unseal, or does it require manual Shamir key entry per customer node?"
- Validates: CP-04 Phase 3 RD=4 (borderline 4-5)
- Source: RP1b (CP-04 Phase 3 Medium confidence)
- Priority: Tier 2

**PE-8. GitOps Architecture Pattern**
"Does the ISV use a shared ArgoCD/Flux hub-and-spoke model (one controller managing all customer clusters) or dedicated GitOps controller instances per customer? Hub-and-spoke holds TE 3; per-customer controller instances add operational burden that would support TE 4."
- Validates: CP-06 Phase 3 TE=3 (borderline TE=3-4)
- Source: RP1b (CP-06 Phase 3 Medium confidence)
- Priority: Tier 3

---

### Role 3: Data Platform Engineer / DBA (7 questions)

**DE-1. Data Service Deployment Pattern** [Top 10 Q5]
"When you deploy your data stack to a new customer environment, how much of the work is parameter-file changes versus active investigation of the customer's storage controller, memory profile, or network topology?"
- Validates: DS1/DS6/DS8/DS9 Phase 2 TE=2, hidden Phase 1 template dependency
- Source: SL1b Section 5.3 (RP3c)
- Priority: Tier 1

**DE-2. Kafka Broker Reconfiguration**
"Have you encountered customers where the disk configuration or node topology required non-trivial Kafka broker reconfiguration? How long did that take?"
- Validates: DS6 Phase 2 RD=2/TE=2, partition strategy recalculation scope
- Source: SL1b Section 5.3 (RP3c)
- Priority: Tier 2

**DE-3. Vector Index Tuning Per Customer**
"For customers with different data volumes, how much time does vector index HNSW parameter tuning add per customer deployment? Have you encountered recall degradation requiring full re-index?"
- Validates: DS8 Phase 2 RD=2/TE=2 (M confidence), HNSW re-index risk (3-6 hrs/160M vectors)
- Source: SL1b Section 5.3 (RP3c)
- Priority: Tier 2

**DE-4. GPU-Tier Embedding Pipeline Tuning**
"For customers with different GPU generations (L4 vs. A100 vs. H100), how much time does embedding pipeline tuning add per customer deployment?"
- Validates: DS9 Phase 2 RD=2/TE=2 (M confidence), GPU-tier template dependency
- Source: SL1b Section 5.3 (RP3c)
- Priority: Tier 2

**DE-5. Configuration Templates**
"Does your team have pre-built configuration templates for different hardware tiers, or do you calibrate parameters fresh for each customer? How does that choice affect per-customer deployment time?"
- Validates: All P3 Phase 2 ratings, hidden Phase 1 dependency on templates
- Source: SL1b Section 5.3 (RP3c)
- Priority: Tier 2

**DE-6. Vector DB Version Upgrades**
"How much engineering time do teams actually spend on Milvus or Qdrant version upgrades per release cycle, particularly for the Woodpecker WAL migration in Milvus 2.6 and index re-optimization after HNSW parameter changes?"
- Validates: DS8 Phase 3 TE=3 (borderline TE=3-4)
- Source: SL1a Section 4 (RP3b); SL1c Section 6
- Priority: Tier 2

**DE-7. Search Engine Operator Overhead**
"How much additional overhead does the Elastic Cloud on Kubernetes (ECK) operator add versus standalone Elasticsearch in managed K8s deployments -- specifically for rolling upgrades and persistent volume management?"
- Validates: DS7 Phase 3 RD=3/TE=3, operator abstraction benefit
- Source: RP3b (DS7 Medium confidence)
- Priority: Tier 3

---

### Role 4: SRE / AI Platform Engineer (6 questions)

**SR-1. Model Routing Maintenance**
"How many hours per week does your team spend updating routing configurations and health check thresholds in response to customer-side inference endpoint changes? Is this growing linearly with customer count?"
- Validates: S6 Phase 3 TE=2 (at-scale caveat: TE=3 at N=50), S6 N-scaling
- Source: SL1a Section 4 (RP4a); SL1b Section 5.4 (RP4a)
- Priority: Tier 2

**SR-2. Inference Quality Monitoring**
"How much time per week does your team spend tracking inference quality and TTFT compliance across customer endpoints -- specifically excluding GPU-level monitoring that the customer owns? Does that effort scale linearly with customer count, or does automation absorb most of it?"
- Validates: S7 Phase 3 TE=2 (at-scale caveat: TE=3 at N=50), S7 N-scaling
- Source: SL1a Section 4 (RP4a); SL1c Section 6
- Priority: Tier 2

**SR-3. Customer Communication Overhead**
"How many hours per week does the team spend coordinating deployment windows with customer IT teams, versus executing the actual deployment?"
- Validates: CC5 customer communication overhead gap, CP-07 Phase 2/Phase 3 scope
- Source: SL1c Section 6 (CC5)
- Priority: Tier 2

**SR-4. CVE Triage Burden**
"What fraction of CP-10 Phase 3 effort is CVE triage and emergency patching across N customer environments, versus routine security operations?"
- Validates: CP-10 Phase 3 TE=4 (proposed adjustment to TE=5), CC5 supply chain security gap
- Source: SL1c Section 6 (CC5)
- Priority: Tier 2

**SR-5. SIEM Integration Per Customer**
"When integrating with a customer's existing SIEM -- Splunk, QRadar, or similar -- how many engineer-days does the Falco-to-SIEM integration, alert rule tuning, and false positive suppression require per customer?"
- Validates: CP-10 Phase 2 TE=2 (annotation: TE=3 when SIEM integration required)
- Source: SL1b Section 5.4 (RP1d)
- Priority: Tier 2

**SR-6. Observability FTE Deduplication**
"Does the 4.6-7.0 FTE range for observability represent dedicated observability headcount or is it partially shared with CP-10 security operations (given that Falco logs and SIEM events feed into the same observability stack)?"
- Validates: CP-05 Phase 3 TE=5 (borderline TE=4-5), FTE deduplication methodology
- Source: RP1b (CP-05 Phase 3 Medium confidence)
- Priority: Tier 3

---

### Role 5: Principal / Staff Engineer (5 questions)

**PS-1. AI Orchestration Stack Coordination** [Top 10 Q6]
"When you upgrade LangGraph or an MCP server version in a customer's on-premises deployment, how many other software components in the AI orchestration stack (Temporal, Langfuse, LiteLLM, guardrails) require coordinated testing or updates in the same maintenance window? Does this coordination work fall on the application team or the deployment team?"
- Validates: AL09 Phase 3 RD=3 (proposed RD=4), CP-07 boundary
- Source: RP2b (AL09 Phase 3 Medium confidence)
- Priority: Tier 1

**PS-2. S1 API Migration Effort**
"When your team replaced Bedrock or Azure OpenAI calls with your customer's vLLM endpoints, how long did it take to re-implement authentication, update error handling, and validate response schema compatibility? Was it days, weeks, or months?"
- Validates: S1 Phase 1 RD=1 (proposed RD=2), auth re-implementation scope
- Source: SL1a Section 4 (RP4a)
- Priority: Tier 2

**PS-3. Message Broker Technology**
"Is the production message broker RabbitMQ Quorum Queues or NATS JetStream?"
- Validates: DS5 Phase 3 TE=2 (conditional: TE=3 if RabbitMQ)
- Source: SL1c Section 6
- Priority: Tier 2

**PS-4. NoSQL Technology Selection**
"What is the primary NoSQL technology -- MongoDB replica sets or Cassandra?"
- Validates: DS2 Phase 3 TE=2 (proposed TE=3 for MongoDB)
- Source: SL1c Section 6
- Priority: Tier 2

**PS-5. AL09 Phase 3 vs. Phase 1 Difficulty**
"Does AL09 (AI/ML orchestration) Phase 3 difficulty genuinely exceed Phase 1 difficulty given the same ecosystem velocity applies to both?"
- Validates: AL09 cross-phase trajectory (only subsegment where P3 RD > P1 RD)
- Source: SL1c Section 6 (CC3 trajectory finding)
- Priority: Tier 3

---

### Role 6: Compliance / Security Lead (2 questions)

**CS-1. Regulatory Variance Across Customers**
"Have two customers' regulatory requirements ever been mutually incompatible in a single software configuration, requiring feature flags or deployment variants?"
- Validates: CC5 regulatory variance gap, CP-09 Phase 3 scope adequacy
- Source: SL1c Section 6 (CC5)
- Priority: Tier 2

**CS-2. SBOM Delivery Burden**
"Do you currently produce and deliver SBOMs for all distributed open-source components? What is the FTE burden?"
- Validates: CC5 supply chain security gap, CP-06/CP-10 Phase 3 scope
- Source: SL1c Section 6 (CC5)
- Priority: Tier 3

---

### Role 7: Backend Engineer (4 questions)

**BE-1. Temporal and DLQ Ownership** [Top 10 Q10]
"For your on-premises Temporal deployment, how frequently do Temporal version upgrades require changes to your workflow SDK code or configuration files? And who owns your dead-letter queue handling code -- platform engineers or backend engineers?"
- Validates: AL05 Phase 3 RD=2 (proposed RD=3), Temporal config drift
- Source: RP2a (AL05 Phase 3 Medium confidence)
- Priority: Tier 2

**BE-2. API Gateway Ownership Split**
"In your on-premises deployments, what fraction of your API gateway maintenance work (Kong/Traefik plugin upgrades, TLS rotation, routing config updates) is handled by platform engineers vs. backend engineers? This split determines whether it belongs in P1 or AL03."
- Validates: AL03 Phase 3 RD=2/TE=2, P1-P2 scope boundary for gateway work
- Source: RP2a (AL03 Phase 3 Medium confidence)
- Priority: Tier 2

**BE-3. Patroni Connection Topology**
"For your on-premises PostgreSQL HA setup with Patroni, how many hours per month does your backend engineering team (not platform team) spend handling connection topology changes, Sentinel failover events, and related ORM-layer issues?"
- Validates: AL04 Phase 3 TE=2 (proposed TE=3), Patroni event handling overhead
- Source: RP2a (AL04 Phase 3 Medium confidence)
- Priority: Tier 2

**BE-4. Startup Sequence Adaptation**
"When you moved your application to on-premises, did you have to write infrastructure-readiness logic into application startup? How many person-days did that add, and which team owned it -- platform or application?"
- Validates: AL02 Phase 1 RD=1 (caveat: startup-sequence code delta)
- Source: RP2a (AL02 Phase 1 minor caveat)
- Priority: Tier 3

---

## Question Priority Summary

### Tier 1: Changes 3+ Ratings If Answered (10 questions)

These are the Top 10 Must-Ask questions listed above. They collectively validate:
- Phase 1 aggregate effort (72-149 person-months)
- Phase 2 per-customer effort segmentation (standard vs. air-gapped vs. regulated)
- Phase 3 grand total FTE (51-95 FTE)
- P2 systematic divergence (+0.9 TE-RD gap)
- 4 proposed rating adjustments (CP-03 TE, AL09 RD, CP-06 TE, AL05 RD)
- Hidden Phase 1 template dependencies for P3 Phase 2

### Tier 2: Changes 1-2 Ratings If Answered (17 questions)

| ID | Question Summary | Validates | Source |
|---|---|---|---|
| PE-3 | PKI/HSM integration time | CP-04 Phase 2 annotation | RP1d |
| PE-5 | Maintenance window negotiation | CP-07 Phase 2 annotation | RP1d |
| PE-6 | FedRAMP vs. SOC 2 delta | CP-09 Phase 2 annotation | RP1d |
| PE-7 | Vault seal/unseal automation | CP-04 Phase 3 RD borderline | RP1b |
| DE-2 | Kafka broker reconfiguration | DS6 Phase 2 TE=2 | RP3c |
| DE-3 | Vector index HNSW tuning | DS8 Phase 2 TE=2 | RP3c |
| DE-4 | GPU-tier embedding tuning | DS9 Phase 2 TE=2 | RP3c |
| DE-5 | Configuration templates | All P3 Phase 2 ratings | RP3c |
| DE-6 | Vector DB version upgrades | DS8 Phase 3 TE borderline | RP3b |
| SR-1 | Model routing maintenance | S6 Phase 3 TE at-scale | RP4a |
| SR-2 | Inference quality monitoring | S7 Phase 3 TE at-scale | RP4a |
| SR-3 | Customer communication overhead | CC5 structural gap | CC5 |
| SR-4 | CVE triage burden | CP-10 Phase 3 TE | CC5 |
| SR-5 | SIEM integration per customer | CP-10 Phase 2 annotation | RP1d |
| PS-2 | S1 API migration effort | S1 Phase 1 RD adjustment | RP4a |
| PS-3 | Message broker technology | DS5 Phase 3 TE conditional | SL1c |
| PS-4 | NoSQL technology selection | DS2 Phase 3 TE adjustment | SL1c |
| CS-1 | Regulatory variance | CC5 gap validation | CC5 |

### Tier 3: Validation Only (8 questions)

| ID | Question Summary | Validates | Source |
|---|---|---|---|
| VE-6 | Organizational structure | CC5 org change gap | CC5 |
| VE-7 | Concurrent K8s versions | CP-07 Phase 3 assumption | SL1c |
| PE-8 | GitOps architecture pattern | CP-06 Phase 3 TE borderline | RP1b |
| DE-7 | ECK operator overhead | DS7 Phase 3 TE | RP3b |
| SR-6 | Observability FTE deduplication | CP-05 Phase 3 TE | RP1b |
| PS-5 | AL09 P3 vs. P1 difficulty | AL09 cross-phase trajectory | CC3 |
| CS-2 | SBOM delivery burden | CC5 supply chain gap | CC5 |
| BE-4 | Startup sequence adaptation | AL02 Phase 1 caveat | RP2a |

---

## Minimum Viable Interview Set

If only 3 interviews are possible (approximately 3 hours total), conduct:

1. **VP Engineering / CTO (60 min):** Ask VE-1 (aggregate Phase 1 effort), VE-2 (Phase 3 FTE headcount), VE-3 (time-to-hire), VE-8 (IAM FTE at scale), VE-5 (SOC scope). Covers Top 10 Q1, Q4, Q7, Q8.

2. **Platform Engineer / DevOps Lead (60 min):** Ask PE-1 (per-customer K8s config), PE-2 (network config time), PE-4 (air-gapped multiplier), PE-3 (PKI integration), PE-7 (Vault seal/unseal). Covers Top 10 Q2, Q3, Q9.

3. **Data Platform Engineer / DBA (45 min):** Ask DE-1 (data service deployment pattern), DE-5 (configuration templates), DE-2 (Kafka reconfiguration), DE-3 (vector index tuning). Covers Top 10 Q5.

This minimum set addresses 9 of the Top 10 questions (missing Q6 and Q10, which require a Principal Engineer or Backend Engineer session).

If a fourth interview is possible:

4. **Principal / Staff Engineer (60 min):** Ask PS-1 (AI orchestration coordination), BE-1 (Temporal/DLQ ownership), PS-2 (S1 API migration), PS-3 (message broker technology). Covers Top 10 Q6 and Q10.

---

## Cross-Reference: Questions to Proposed Rating Adjustments

The following table maps each proposed rating adjustment from SL1a, SL1b, and SL1c to the specific interview question(s) that would confirm or refute it.

| Proposed Adjustment | Direction | Source | Interview Question(s) |
|---|---|---|---|
| S1 Phase 1 RD: 1 -> 2 | Upward | RP4a | PS-2 |
| CP-03 Phase 3 TE: 4 -> 5 | Upward | RP1a, SL1c | VE-8 |
| CP-06 Phase 3 TE: 3 -> 4 | Upward | RP1b, SL1c | PE-8 |
| CP-08 Phase 3 TE: 3 -> 4 | Upward | SL1c | VE-2 (indirect) |
| CP-10 Phase 3 TE: 4 -> 5 | Upward | SL1c | SR-4 |
| AL04 Phase 3 TE: 2 -> 3 | Upward | RP2a | BE-3 |
| AL05 Phase 3 RD: 2 -> 3 | Upward | RP2a | BE-1 (Q10) |
| AL09 Phase 3 RD: 3 -> 4 | Upward | RP2b | PS-1 (Q6) |
| DS2 Phase 3 TE: 2 -> 3 | Upward | RP3a, SL1c | PS-4 |
| DS5 Phase 3 TE: 2 -> 3 | Conditional (RabbitMQ) | RP3a, SL1c | PS-3 |
| P3 FTE aggregate: ~10-18 -> 11.85-19.55 | Correction | CC1, SL1c | DE-1 (indirect) |
| Phase 2 total: 10-21 pw -> 12-25 pw | Upward | RP1d, RP3c | PE-1, PE-2, DE-1 |

---

## Sources

### Synthesis Layer Files (Primary Interview Question Sources)

| File | Absolute Path | Questions Contributed |
|---|---|---|
| SL1a_phase1_consolidated.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/SL1a_phase1_consolidated.md` | 14 Phase 1 questions (Section 4) |
| SL1b_phase2_consolidated.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/SL1b_phase2_consolidated.md` | 21 Phase 2 questions (Section 5) |
| SL1c_phase3_consolidated.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/SL1c_phase3_consolidated.md` | 15 Phase 3 questions (Section 6) |

### Review Agent Files (Original Question Generators)

| File | Absolute Path | Questions Contributed |
|---|---|---|
| RP1a_P1_infrastructure_core.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/RP1a_P1_infrastructure_core.md` | CP-01/CP-02/CP-03 Phase 2 & 3 interview questions |
| RP1b_P1_security_delivery.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/RP1b_P1_security_delivery.md` | CP-04/CP-05/CP-06 Phase 3 interview questions |
| RP2a_P2_service_architecture.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/RP2a_P2_service_architecture.md` | AL02/AL03/AL04/AL05 interview questions |
| RP2b_P2_resilience_ai_testing.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/RP2b_P2_resilience_ai_testing.md` | AL07/AL09/AL10 interview questions |
| RP3a_P3_traditional_data.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/RP3a_P3_traditional_data.md` | DS1-DS5 interview questions |
| RP3b_P3_streaming_ai_data.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/RP3b_P3_streaming_ai_data.md` | DS7/DS8 interview questions |
| RP3c_P3_effort_validation.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/RP3c_P3_effort_validation.md` | P3 Phase 2 aggregate effort questions |
| RP4a_P4_isv_scope.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/RP4a_P4_isv_scope.md` | S1/S6/S7 interview questions |

### Cross-Cutting Review Files

| File | Absolute Path | Contribution |
|---|---|---|
| CC5_missing_risks.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/CC5_missing_risks.md` | 6 structural gap questions (talent, communication, supply chain, regulatory) |
| CC3_cross_phase_consistency.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/review/CC3_cross_phase_consistency.md` | AL09 cross-phase trajectory question |

### Primary File Under Review

| File | Absolute Path |
|---|---|
| three_phase_on_prem_ratings.md | `/Users/nicholaspate/Documents/01_Active/Corp_Strat/ezAI/isv_development/analysis/three_phase_on_prem_ratings.md` |
