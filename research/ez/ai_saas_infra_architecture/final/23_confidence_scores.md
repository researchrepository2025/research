# Final Confidence Scores: AI SaaS Infrastructure Architecture Adoption

**Scoring Date:** 2026-02-12
**Scoring Agent:** Confidence Scoring Agent (Final Arbiter)
**Inputs:** consolidated/18_consolidated_draft.md + verification files 19-22

---

## 1. Executive Summary

Every cell in the 21-cell estimate matrix is classified as Estimated or Inferred; no cell has Direct evidence for the specific research question (AI SaaS architecture choice by revenue tier). The four Wave 3 verification agents collectively identified 36 issues, recommended lowering confidence on 15+ cells, and provided bias-adjusted estimates that shift managed K8s downward by 5-10pp and cloud-native non-K8s upward by a corresponding amount. After weighing all recommendations against the scoring rubric, this report assigns final confidence scores that average 4.1/10 across all 21 cells (draft average was 4.9/10). The largest reductions apply to the sub-$10M managed K8s cell (draft C:5, final C:3) and EU cells (draft C:3, final C:2 for all six). The strongest cell remains Managed K8s $200M+ at C:6, reduced from C:7 because even the best-supported cell relies on extrapolation from a handful of named companies to the full $200M+ population.

---

## 2. Complete Final Confidence Score Matrix

### Cloud-Native (Non-K8s Managed): Serverless, PaaS, ECS/Fargate, Cloud Run

| | <$10M | $10-50M | $50-200M | $200M+ | US Avg | EU Avg | Overall |
|---|---|---|---|---|---|---|---|
| **Final Confidence** | 4 | 4 | 3 | 4 | 4 | 2 | 3 |

### Managed Kubernetes (EKS, AKS, GKE)

| | <$10M | $10-50M | $50-200M | $200M+ | US Avg | EU Avg | Overall |
|---|---|---|---|---|---|---|---|
| **Final Confidence** | 3 | 5 | 5 | 6 | 4 | 2 | 4 |

### Open/Self-Managed Kubernetes

| | <$10M | $10-50M | $50-200M | $200M+ | US Avg | EU Avg | Overall |
|---|---|---|---|---|---|---|---|
| **Final Confidence** | 4 | 4 | 3 | 5 | 3 | 2 | 3 |

**Matrix Summary Statistics:**
- Highest confidence: Managed K8s $200M+ (C:6)
- Lowest confidence: All EU Avg cells (C:2)
- Mean confidence: 3.5
- Median confidence: 4
- Cells at C:5 or above: 4 of 21 (19%)
- Cells at C:3 or below: 10 of 21 (48%)

---

## 3. Justification Table

### Cloud-Native (Non-K8s Managed)

| Cell | Draft Score | Agent 19 (Triangulation) Rec | Agent 20 (Contradictions) Rec | Agent 21 (Bias) Rec | Agent 22 (Gaps) Rec | Final Score | Justification |
|---|---|---|---|---|---|---|---|
| **<$10M** | C:6 | Retain C:6 (range widened to 60-80%) | No change | **Drop to C:3** (zero empirical data for <500 employee companies [SOURCE: [Command Linux K8s Statistics](https://commandlinux.com/statistics/linux-container-kubernetes-adoption-statistics/)]; estimate is pure extrapolation from enterprise data) | **Drop to C:5** (defined by exclusion, not measurement; relies on inverting enterprise K8s data) | **C:4** | Agent 21's C:3 is too aggressive -- VC stage guidance ([Maven Solutions](https://www.mavensolutions.tech/blog/cloud-infrastructure-on-a-startup-budget), [SaaStr](https://www.saastr.com/is-there-a-benchmark-for-of-revenue-that-an-enterprise-saas-business-should-spend-on-systems-infrastructures-like-aws-or-the-equivalent/), YC patterns [SOURCE NEEDED]) from multiple independent advisory sources provides genuine directional signal, which the rubric rates as "Weak evidence, Mostly Inferred/Estimated" (C:4-5). However, Agent 22 correctly notes this category is measured by exclusion, and Agent 19 correctly notes the VC guidance is prescriptive not descriptive. The bimodality within the tier (pre-revenue at ~100% non-K8s, $5-10M at maybe 40-50% non-K8s) makes a single confidence score misleading. Final C:4 reflects moderate evidence for direction but weak evidence for the specific range. |
| **$10-50M** | C:5 | No specific recommendation | **Drop to C:4** (Weak density + Estimated + largest conflict in the log at 15-30pp delta requiring definitional reframing) | **Drop to C:4** (residual estimation from K8s data that carries significant bias; no direct non-K8s measurement) | Retain C:5 (30pp conflict partially mitigated by definitional clarification) | **C:4** | Agents 20 and 21 converge on C:4. The draft's C:5 is generous for a cell with Weak evidence density, Estimated classification, and the single largest conflict resolution in the entire conflict log. The 30-45% range was derived by introducing a new definitional category ("primary or significant") that neither source agent used. Per the rubric: "Weak evidence. Mostly Inferred/Estimated. Significant bias concerns or single-source" = C:4-5. The definitional ambiguity and bias concerns push to C:4. |
| **$50-200M** | C:4 | No change | No change | **Drop to C:3** (no direct survey data; existence proof from case studies but not prevalence) | Retain C:4 (Weak evidence density acknowledged) | **C:3** | Agent 21's argument is persuasive: this cell has Weak evidence density, Estimated classification, and the estimate is derived as a residual from K8s data. No named AI SaaS company at this tier is documented as using non-K8s as primary architecture. Per the rubric: "Very weak. Primarily Estimated with limited supporting data. Major assumptions required" = C:2-3. The existence of some indirect supporting data from the [Datadog State of Containers and Serverless 2025](https://www.datadoghq.com/state-of-containers-and-serverless/) telemetry and tier agent analysis lifts it above C:2 to C:3. |
| **$200M+** | C:5 | No change | **Drop to C:4** (second-largest conflict at 25-40pp; resolution effectively overrides Agent 09 entirely; evidence for non-K8s inferred from complement of K8s) | **Drop to C:4** (inference from absence is weak evidence) | **Drop to C:4** (no named AI SaaS company uses non-K8s as primary at this tier) | **C:4** | Three of four agents agree on C:4. The draft's C:5 was inflated because the cell has Moderate evidence density, but that density comes from K8s adoption data (telling us what companies DO use), not from non-K8s measurement. Agent 20 correctly notes the conflict resolution was unusually aggressive (discarding Agent 09's entire range). Per the rubric: C:4-5 for mix of Inferred and Estimated with some bias concerns. The resolution-quality concerns push to C:4. |
| **US Avg** | C:5 | No change | No change | **Drop to C:4** (US serverless data is for "any use" not primary; no direct primary-use measurement) | Retain C:5 | **C:4** | Agent 21's point is well-taken: the [Datadog Lambda 65% figure](https://www.datadoghq.com/blog/containers-and-serverless-2025-study-learnings/) measures any invocation, not primary architecture. Agent 16's estimate (30-40%) required reframing from "some workloads" at 45-55%. This reframing is judgment-based, adding uncertainty. Per the rubric: C:4-5 for Estimated with moderate bias concerns. The reframing step and lack of direct primary-use measurement push to C:4. |
| **EU Avg** | C:3 | No change (EU remains C:3; Agent 19 notes [CNCF 82% EU cloud-native](https://www.cncf.io/reports/cncf-annual-survey-2023/) measures development intensity, not K8s) | No change | **Drop to C:2** (no EU-specific non-K8s data whatsoever; estimate is US data adjusted by structural GDPR argument with zero empirical support) | Retain C:3 | **C:2** | Agent 21 is correct: there is literally zero empirical data for EU non-K8s adoption among AI SaaS companies. The estimate is entirely derived from the US estimate minus a structural GDPR adjustment. Per the rubric: "Very weak. Primarily Estimated with limited supporting data. Major assumptions required" = C:2-3. The complete absence of any EU-specific non-K8s data pushes to C:2. I disagree with Agent 22's retention of C:3 -- a C:3 implies "at least one weak source" exists, but none does for this specific cell. |
| **Overall** | C:4 | No change | No change (add methodology note about weighting) | **Drop to C:3** (structural under-measurement of non-K8s; all estimates derived as residuals from biased K8s data) | Retain C:4 | **C:3** | Agent 21's argument about structural under-measurement is convincing: every data source measures K8s better than non-K8s, so the residual-based non-K8s estimates inherit inverted K8s bias. Agent 20 also identifies that the "Overall" weighting methodology is unstated, adding another layer of uncertainty. Per the rubric: C:3 for primarily Estimated classification with significant structural bias concerns. |

### Managed Kubernetes (EKS, AKS, GKE)

| Cell | Draft Score | Agent 19 (Triangulation) Rec | Agent 20 (Contradictions) Rec | Agent 21 (Bias) Rec | Agent 22 (Gaps) Rec | Final Score | Justification |
|---|---|---|---|---|---|---|---|
| **<$10M** | C:5 | **Drop to C:4** (bimodality within tier makes single range unreliable; VC guidance prescriptive not descriptive) | No change | **Drop to C:3** ([CNCF data](https://www.cncf.io/reports/cncf-annual-survey-2024/) inapplicable below 500 employees [SOURCE: [Command Linux K8s Statistics](https://commandlinux.com/statistics/linux-container-kubernetes-adoption-statistics/)]; VC stage guidance contradicts the estimate; no empirical data for this tier; "largest over-confidence in the matrix") | Retain C:5 (wide range appropriately reflects uncertainty) | **C:3** | This is the cell with the most disagreement among agents. Agent 21's C:3 is aggressive but defensible: [CNCF survey data](https://www.cncf.io/reports/cncf-annual-survey-2024/) excludes companies under 500 employees (the entire sub-$10M tier) [SOURCE: [Command Linux K8s Statistics](https://commandlinux.com/statistics/linux-container-kubernetes-adoption-statistics/)], the only tier-specific data is prescriptive VC guidance recommending AGAINST K8s, and there are essentially zero empirical measurements. Agent 19 agrees the confidence is inflated. Agent 22 retains C:5 based on range width, but range width compensates for uncertainty in the estimate, not for absence of data. Per the rubric: "Very weak. Primarily Estimated with limited supporting data. Major assumptions required" = C:2-3. There is one supporting data point ([Stacks on GKE Autopilot](https://cloud.google.com/customers/stacks), [Sonantic on EKS](https://www.bionconsulting.com/case-studies/scaling-ai-driven-voice-technology-with-aws-and-eks)) but these are individual anecdotes. C:3 is appropriate. I disagree with Agent 22's C:5 retention -- the evidence base for this specific cell is among the weakest in the matrix. |
| **$10-50M** | C:6 | No specific change (notes API-wrapper caveat) | No change (notes intersection logic should narrow to 55-65%) | **Drop to C:4** ([CNCF selection bias](https://www.cncf.io/reports/cncf-annual-survey-2024/) 26-28pp not fully discounted [SOURCE: [Gartner K8s Adoption](https://www.gartner.com/en/documents/5405263) vs CNCF gap]; job posting data measures demand not deployment; no direct survey) | Retain C:6 (two agents converge; case study support) | **C:5** | Agent 21's C:4 is too aggressive for this cell. Two independent agents (10 and 13) converge at overlapping ranges, which per the rubric is "Strong evidence from 2+ sources, mostly Direct/Inferred, minor bias concerns" (C:8-9) -- except the sources are Estimated, not Direct/Inferred, and CNCF bias remains a concern. The convergence of two independent analytical agents, even on Estimated data, provides genuine triangulation. Per the rubric: C:5-6 for "Moderate evidence, mix of Direct and Inferred, some bias concerns." The bias concerns ([CNCF](https://www.cncf.io/reports/cncf-annual-survey-2024/), [Dynatrace](https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/)) pull toward C:5; the agent convergence pulls toward C:6. Final C:5 splits the difference. I disagree with Agent 21's 2-point reduction -- agent convergence has value even when underlying data is biased. |
| **$50-200M** | C:6 | No change | No change | **Drop to C:5** (Inferred classification appropriate but inference chain starts from biased [CNCF data](https://www.cncf.io/reports/cncf-annual-survey-2024/)) | **Drop to C:5** (should reclassify from Inferred to Estimated; chain from general K8s to AI SaaS managed K8s is assumption-dependent) | **C:5** | Agents 21 and 22 converge on C:5. Agent 22's argument about reclassification from Inferred to Estimated is persuasive: the inference chain requires an untested assumption (X2: AI SaaS is more cloud-native than average enterprise) that makes it closer to Estimated than Inferred. Agent 21 correctly notes the [Dynatrace telemetry](https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/) carries its own bias toward observability-mature companies. Per the rubric: C:5 for "Weak evidence, Mostly Inferred/Estimated" -- but with reasonable supporting evidence from case studies ([Figma](https://www.figma.com/blog/migrating-onto-kubernetes/), [Grammarly](https://www.grammarly.com/blog/engineering/ml-infrastructure-research-experimentation/), [Notion](https://www.notion.com/blog/building-and-scaling-notions-data-lake)), pushing toward the upper end of this band. |
| **$200M+** | C:7 | No change | No change | **Drop to C:6** (named company evidence is genuine but represents top-of-market; small sample extrapolation) | Retain C:7 (strongest cell in matrix; named company examples) | **C:6** | Agent 21's C:6 is accepted. This is the strongest cell in the matrix, with multiple named companies ([Anthropic on GKE](https://cloudnativenow.com/editorial-calendar/best-of-2025/how-anthropic-dogfoods-on-claude-code-2/), [Snowflake on EKS](https://aws.amazon.com/blogs/architecture/snowflake-running-millions-of-simulation-tests-with-amazon-eks/), [Figma on EKS](https://www.figma.com/blog/migrating-onto-kubernetes/), [Notion on EKS](https://www.notion.com/blog/building-and-scaling-notions-data-lake), [Grammarly on EKS](https://www.grammarly.com/blog/engineering/ml-infrastructure-research-experimentation/)), convergent [CNCF](https://www.cncf.io/reports/cncf-annual-survey-2024/)/[Dynatrace](https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/) data, and SEC filing infrastructure signals. However, Agent 21 correctly notes these are predominantly $1B+ ARR companies, not representative of the full $200M+ tier. Per the rubric: C:8-9 requires "Strong evidence from 2+ sources, mostly Direct/Inferred, minor bias concerns." The evidence is strong but all Inferred or Estimated for AI SaaS specifically (no source directly segments AI SaaS managed K8s at this tier). The named companies are Direct existence proofs but not Direct prevalence measurements. C:6-7 is the appropriate range. Agent 21's sample-size concern tips to C:6. I disagree with Agent 22's C:7 retention -- the rubric requires "2+ sources, mostly Direct/Inferred" for C:8-9, and this cell does not reach that bar. |
| **US Avg** | C:5 | No change | No change | **Drop to C:4** (CNCF-Gartner gap calibration not fully applied [SOURCE: [CNCF](https://www.cncf.io/reports/cncf-annual-survey-2024/) vs [Gartner](https://www.gartner.com/en/documents/5405263) 26-28pp gap]) | Retain C:5 | **C:4** | Agent 21's argument is sound: the CNCF-Gartner gap (26-28pp measured, [Gartner 54%](https://www.gartner.com/en/documents/5405263) vs [CNCF ~82%](https://www.cncf.io/reports/cncf-annual-survey-2024/)) is the best available calibration tool for K8s bias, and the draft's 15-25pp discount underuses it. The US Avg cell also required reframing from Agent 16's "some workloads" definition. Per the rubric: C:4-5 for Estimated with moderate bias concerns. The under-applied bias discount pushes to C:4. |
| **EU Avg** | C:3 | No change | No change | **Drop to C:2** (two EU case studies is anecdotal, not evidence; GDPR-driven K8s argument is plausible but unquantified) | Retain C:3 | **C:2** | Same logic as Cloud-native EU Avg. Agent 17 self-reports 3/10 confidence. Only 2 EU case studies exist in the entire Wave 1 dataset ([Stacks](https://cloud.google.com/customers/stacks), [Medigold Health](https://azure.microsoft.com/en-us/blog/scaling-generative-ai-in-the-cloud-enterprise-use-cases-for-driving-secure-innovation/)). The AKS EU enterprise strength claim is extrapolated from general enterprise data, not AI SaaS data. Per the rubric: C:2 for "Very weak, primarily Estimated, major assumptions required." The GDPR structural argument is plausible but completely unquantified. |
| **Overall** | C:5 | **Drop to C:4** (wider uncertainty range on managed share from conflicting sources at [61% Tigera](https://www.tigera.io/learn/guides/kubernetes-security/kubernetes-statistics/)-[73% Dynatrace](https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/)) | No change (add weighting methodology note) | **Drop to C:4** (structural upward bias from all sources; managed K8s still likely largest category but at lower adoption) | Retain C:5 | **C:4** | Agents 19 and 21 converge on C:4. The conflicting managed K8s share data ([61% Tigera](https://www.tigera.io/learn/guides/kubernetes-security/kubernetes-statistics/), 67% cloud-hosted [SOURCE NEEDED], [73% Dynatrace](https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/)) creates a wider uncertainty band than the draft acknowledges. Agent 20 identifies the unstated weighting methodology as an additional uncertainty source. Per the rubric: C:4-5 for Estimated with bias concerns. The compounding of managed-share uncertainty + weighting ambiguity + structural upward bias pushes to C:4. |

### Open/Self-Managed Kubernetes

| Cell | Draft Score | Agent 19 (Triangulation) Rec | Agent 20 (Contradictions) Rec | Agent 21 (Bias) Rec | Agent 22 (Gaps) Rec | Final Score | Justification |
|---|---|---|---|---|---|---|---|
| **<$10M** | C:5 | No change | No change | **Drop to C:4** (2-5% likely directionally correct but even this may overstate; lower bound could be 0-1%) | Retain C:5 (strong structural argument for very low adoption) | **C:4** | Agent 21's point has merit: even the 2-5% range may be generous for a tier where companies have 5-30 engineers. No company with a 5-person engineering team self-manages Kubernetes. The structural argument is strong (providing directional confidence) but the specific range is based on zero empirical measurements. Per the rubric: C:4 for "Weak evidence, Mostly Estimated, significant assumptions." The structural argument about team size is sound but is a single inferential chain, not multiple independent sources. |
| **$10-50M** | C:5 | No change | **Drop to C:4** (Weak evidence density + Estimated + source estimate covered different tier boundary) | **Drop to C:4** (derived from [CNCF data](https://www.cncf.io/reports/cncf-annual-survey-2024/) that over-represents K8s enthusiasts; some self-managed CNCF respondents may be hobbyists/educators) | Retain C:5 | **C:4** | Agents 20 and 21 converge on C:4. The draft rated this Weak evidence density with Estimated classification, which is inconsistent with C:5 per the draft's own framework. Agent 11's original estimate covered $10-200M combined (not this specific tier), and Agent 13 (preferred source) analyzed this tier specifically but self-reported only 5/10 confidence overall. Per the rubric: C:4 for Weak/Estimated with the additional concern that the tier boundary was adjusted. |
| **$50-200M** | C:4 | No change | No change | **Drop to C:3** (no direct data; estimate split from total K8s figure that is itself biased upward) | Retain C:4 | **C:3** | Agent 21 is persuasive: this cell is derived by subtracting managed K8s from total K8s at this tier, both of which carry upward bias. No named self-managed K8s company exists at this specific tier ([OpenAI](https://openai.com/index/scaling-kubernetes-to-7500-nodes/), [Databricks](https://www.databricks.com/blog/managing-cicd-kubernetes-authentication-using-operators), [Salesforce](https://engineering.salesforce.com/tagged/kubernetes/) are all $200M+). Agent 11's estimate covered $10-200M combined, not this tier specifically. Per the rubric: C:2-3 for "Very weak, primarily Estimated, major assumptions required." The existence of a reasonable structural argument (some companies at this tier have enough platform engineers to self-manage) lifts above C:2 to C:3. |
| **$200M+** | C:6 | **Drop to C:5-6** ([OpenAI](https://openai.com/index/scaling-kubernetes-to-7500-nodes/) and [HubSpot](https://www.cncf.io/case-studies/hubspot/) classifications are ambiguous -- OpenAI may be on AKS now given the [$250B Azure commitment](https://openai.com/index/next-chapter-of-microsoft-openai-partnership/); HubSpot managed vs self-managed unclear) | No change | **Drop to C:5** (4 named companies at extreme tail of $1B+ ARR; not representative of typical $200-500M company) | Retain C:6 (named company examples provide strong anchor) | **C:5** | Agents 19 and 21 converge on C:5. Agent 19's source verification reveals that only 2 of 4 named "self-managed" companies are clearly confirmed: [Salesforce](https://engineering.salesforce.com/tagged/kubernetes/) (bare metal K8s, definitive) and [Databricks](https://www.databricks.com/blog/managing-cicd-kubernetes-authentication-using-operators) (hybrid, confirmed). [OpenAI's 7,500-node cluster](https://openai.com/index/scaling-kubernetes-to-7500-nodes/) may now be on AKS given the [$250B Azure commitment](https://openai.com/index/next-chapter-of-microsoft-openai-partnership/). [HubSpot's](https://www.cncf.io/case-studies/hubspot/) classification is ambiguous. Per the rubric: C:5 for "Moderate evidence, mix of sources, some bias concerns." Two confirmed companies ([Salesforce](https://engineering.salesforce.com/tagged/kubernetes/), [Databricks](https://www.databricks.com/blog/managing-cicd-kubernetes-authentication-using-operators)) plus the [Dynatrace 27% self-managed](https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/) baseline provide moderate evidence. I disagree with Agent 22's C:6 retention -- the rubric requires stronger evidence for C:6 than 2 confirmed + 2 ambiguous company examples. |
| **US Avg** | C:4 | No change | No change | **Drop to C:3** (no direct US-specific self-managed K8s data; derived from total K8s minus managed K8s, both biased) | Retain C:4 | **C:3** | Agent 21's argument is sound: there is no US-specific self-managed K8s data for AI SaaS. The estimate is derived from the difference between total K8s (biased upward) and managed K8s (biased upward), with the residual potentially amplifying errors. Per the rubric: C:3 for primarily Estimated with limited supporting data and derivation from biased components. |
| **EU Avg** | C:3 | No change | No change | **Drop to C:2** (GDPR sovereignty argument for higher self-managed K8s is plausible but supported by zero empirical data) | Retain C:3 | **C:2** | Consistent with all other EU cells. The GDPR sovereignty argument is structurally logical but entirely unquantified. The convergence between Agent 11 and Agent 17 at 20-30% is not independent -- both rely on the same GDPR structural reasoning. Per the rubric: C:2 for "Very weak, primarily Estimated, major assumptions required, no independent data." |
| **Overall** | C:4 | No change | No change | **Drop to C:3** (self-managed share declining is well-supported directionally at 7/10; specific percentages are low-confidence) | Retain C:4 | **C:3** | Agent 21 makes a critical distinction: the directional trend (declining self-managed) deserves approximately C:7, but the specific percentage range (15-22%) deserves C:3-4. Since the confidence score applies to the estimate (the percentage), not just the trend direction, C:3 is appropriate. The overall figure also inherits the unstated weighting methodology issue (Agent 20, Issue 6). |

---

## 4. Summary of Disagreements with Wave 3 Agents

### Disagreements with Agent 19 (Triangulation)

None. All Agent 19 recommendations were adopted or directionally consistent with the final scores.

### Disagreements with Agent 20 (Contradictions)

None. All Agent 20 recommendations were adopted. Agent 20 was the most conservative of the four agents (recommending 3 confidence reductions), and all three were accepted.

### Disagreements with Agent 21 (Bias Assessment)

| Cell | Agent 21 Rec | Final Score | Reason for Disagreement |
|---|---|---|---|
| Cloud-native <$10M | C:3 | C:4 | Agent 21 argues "zero empirical data" but overlooks that multiple independent VC advisory sources ([Maven Solutions](https://www.mavensolutions.tech/blog/cloud-infrastructure-on-a-startup-budget), [SaaStr](https://www.saastr.com/is-there-a-benchmark-for-of-revenue-that-an-enterprise-saas-business-should-spend-on-systems-infrastructures-like-aws-or-the-equivalent/), YC batch patterns [SOURCE NEEDED]) provide converging directional signal, even though they are prescriptive rather than descriptive. This convergence across independent advisory sources warrants C:4, not C:3. |
| Managed K8s $10-50M | C:4 | C:5 | Agent 21 applies a 2-point reduction based on [CNCF bias](https://www.cncf.io/reports/cncf-annual-survey-2024/), but two independent analytical agents (10 and 13) converge at overlapping ranges. Agent convergence on Estimated data still provides genuine triangulation value. The rubric's C:6-7 range for 2+ agreeing sources is too high for Estimated data, but the convergence lifts above C:4. |
| Managed K8s $50-200M | C:5 | C:5 | Agreement. No disagreement. |
| Managed K8s $200M+ | C:6 | C:6 | Agreement. No disagreement. |
| Managed K8s <$10M | C:3 | C:3 | Agreement. No disagreement. |
| All EU cells | C:2 | C:2 | Agreement. No disagreement. |

**Note on Agent 21's systematic approach:** Agent 21 recommended reducing ALL 21 cells by 1-2 points, with an average reduction of 1.2 points. This blanket reduction is partially warranted (the unidirectional source bias is real) but partially overcorrects for cells where non-bias evidence exists (e.g., VC stage guidance for <$10M from [Maven Solutions](https://www.mavensolutions.tech/blog/cloud-infrastructure-on-a-startup-budget) and [SaaStr](https://www.saastr.com/is-there-a-benchmark-for-of-revenue-that-an-enterprise-saas-business-should-spend-on-systems-infrastructures-like-aws-or-the-equivalent/), named company examples for $200M+ including [Anthropic](https://cloudnativenow.com/editorial-calendar/best-of-2025/how-anthropic-dogfoods-on-claude-code-2/), [Snowflake](https://aws.amazon.com/blogs/architecture/snowflake-running-millions-of-simulation-tests-with-amazon-eks/), [Figma](https://www.figma.com/blog/migrating-onto-kubernetes/), [Notion](https://www.notion.com/blog/building-and-scaling-notions-data-lake), [Grammarly](https://www.grammarly.com/blog/engineering/ml-infrastructure-research-experimentation/)). The final scores accept Agent 21's reductions for 14 cells and reject or moderate them for 7 cells.

### Disagreements with Agent 22 (Gap Analysis)

| Cell | Agent 22 Rec | Final Score | Reason for Disagreement |
|---|---|---|---|
| Cloud-native <$10M | C:5 | C:4 | Agent 22 reduces by only 1 point (C:6 to C:5), citing the exclusion-based measurement method. However, the bias concerns raised by Agent 21 and the bimodality within the tier (identified by Agent 19) together warrant a larger reduction. The evidence quality for this cell is weaker than Agent 22 acknowledges. |
| Managed K8s <$10M | C:5 (retain) | C:3 | Agent 22 retains C:5, arguing the wide range appropriately reflects uncertainty. This conflates range width with confidence. A wide range compensates for imprecision in the estimate; it does not substitute for the absence of data. The rubric scores evidence quality, not range width. |
| Managed K8s $200M+ | C:7 (retain) | C:6 | Agent 22 retains C:7 as the strongest cell. While this IS the strongest cell, the rubric's C:8-9 requires "Multiple independent Direct sources agree," and this cell has zero Direct sources for the specific research question (AI SaaS managed K8s by revenue tier). Named company examples ([Anthropic](https://cloudnativenow.com/editorial-calendar/best-of-2025/how-anthropic-dogfoods-on-claude-code-2/), [Snowflake](https://aws.amazon.com/blogs/architecture/snowflake-running-millions-of-simulation-tests-with-amazon-eks/), [Figma](https://www.figma.com/blog/migrating-onto-kubernetes/), [Notion](https://www.notion.com/blog/building-and-scaling-notions-data-lake), [Grammarly](https://www.grammarly.com/blog/engineering/ml-infrastructure-research-experimentation/)) are Direct existence proofs, not Direct prevalence measurements. |
| Self-managed K8s $200M+ | C:6 (retain) | C:5 | Agent 22 retains C:6 based on named company anchors. However, Agent 19's verification found that only 2 of 4 named companies are clearly confirmed as self-managed ([Salesforce](https://engineering.salesforce.com/tagged/kubernetes/) and [Databricks](https://www.databricks.com/blog/managing-cicd-kubernetes-authentication-using-operators)). [OpenAI's cluster](https://openai.com/index/scaling-kubernetes-to-7500-nodes/) may now be on AKS given the [$250B Azure commitment](https://openai.com/index/next-chapter-of-microsoft-openai-partnership/), and [HubSpot's](https://www.cncf.io/case-studies/hubspot/) classification is ambiguous. The evidence base is thinner than Agent 22 acknowledges. |
| All EU cells | C:3 (retain) | C:2 | Agent 22 retains C:3 for all EU cells, arguing they are "already at appropriate level." However, C:3 implies at least one weak supporting source exists. For EU-specific AI SaaS architecture data, essentially no source exists -- the estimates are structural arguments applied to US data. Only 2 EU case studies exist ([Stacks](https://cloud.google.com/customers/stacks), [Medigold Health](https://azure.microsoft.com/en-us/blog/scaling-generative-ai-in-the-cloud-enterprise-use-cases-for-driving-secure-innovation/)). C:2 is more accurate per the rubric. |

---

## 5. Meta-Assessment

### Overall Research Quality

The research is substantially more rigorous than typical industry analysis. The explicit classification system (Direct/Inferred/Estimated), the comprehensive assumptions register (30+ documented), the conflict log with transparent resolutions, and the evidence density map are rare in analyst reports. The primary limitation is not analytical rigor but data availability -- the core research question (AI SaaS architecture choice by revenue tier) has never been directly measured by any source.

### Biggest Remaining Uncertainties (Ranked)

1. **API-wrapper vs model-hosting split by tier (impact: +/-10-15pp on all tier cells).** This is the single largest unquantified variable. API-wrapper companies (calling OpenAI/Anthropic APIs) have fundamentally different infrastructure needs than model-hosting companies. The split almost certainly varies dramatically by tier but no source quantifies it.

2. **Sub-$10M tier architecture distribution (impact: all <$10M cells at C:3-4).** Companies under 500 employees are invisible in every survey and telemetry source [SOURCE: [Command Linux K8s Statistics](https://commandlinux.com/statistics/linux-container-kubernetes-adoption-statistics/)]. The <$10M tier is the largest by company count (likely 60%+ of all AI SaaS companies) and the least well-measured. Every estimate for this tier is extrapolation from enterprise data applied downward or qualitative VC guidance ([Maven Solutions](https://www.mavensolutions.tech/blog/cloud-infrastructure-on-a-startup-budget), [SaaStr](https://www.saastr.com/is-there-a-benchmark-for-of-revenue-that-an-enterprise-saas-business-should-spend-on-systems-infrastructures-like-aws-or-the-equivalent/)).

3. **EU architecture patterns (impact: all EU cells at C:2).** Only 2 EU AI company case studies exist ([Stacks](https://cloud.google.com/customers/stacks), [Medigold Health](https://azure.microsoft.com/en-us/blog/scaling-generative-ai-in-the-cloud-enterprise-use-cases-for-driving-secure-innovation/)). The GDPR sovereignty argument is structurally logical but entirely unquantified. The EU AI Act (effective August 2026 [SOURCE NEEDED]) may further differentiate EU patterns, but no compliance-driven architecture data exists yet.

4. **Magnitude of CNCF selection bias (impact: all K8s cells by 5-15pp).** The measured CNCF-Gartner gap is 26-28pp ([CNCF ~82%](https://www.cncf.io/reports/cncf-annual-survey-2024/) vs [Gartner 54%](https://www.gartner.com/en/documents/5405263)). Whether the "true" bias discount for AI SaaS companies should be 15pp, 20pp, or 28pp is unknowable without direct measurement. This single parameter shifts all K8s estimates by a corresponding amount.

5. **Non-K8s primary adoption rate (impact: all cloud-native cells by 5-10pp).** No source directly measures "companies using ECS/Fargate/Cloud Run/Lambda as primary infrastructure." All non-K8s estimates are residuals from K8s data, inheriting inverted K8s bias. A purpose-built [Datadog](https://www.datadoghq.com/state-of-containers-and-serverless/) or AWS telemetry segmentation for container services would transform these cells.

### Directional Findings vs Percentage Estimates

The research is substantially more reliable on directional findings than on specific percentages:

| Directional Finding | Confidence |
|---|---|
| Managed K8s is the dominant architecture at $50M+ ARR (supported by [Dynatrace 73% managed](https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/), [Tigera 61% managed](https://www.tigera.io/learn/guides/kubernetes-security/kubernetes-statistics/), named companies [Anthropic](https://cloudnativenow.com/editorial-calendar/best-of-2025/how-anthropic-dogfoods-on-claude-code-2/), [Snowflake](https://aws.amazon.com/blogs/architecture/snowflake-running-millions-of-simulation-tests-with-amazon-eks/), [Figma](https://www.figma.com/blog/migrating-onto-kubernetes/), [Notion](https://www.notion.com/blog/building-and-scaling-notions-data-lake), [Grammarly](https://www.grammarly.com/blog/engineering/ml-infrastructure-research-experimentation/)) | High (C:7+) |
| Self-managed K8s is declining over time (supported by [Dynatrace 27% self-managed](https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/), [Red Hat survey](https://www.redhat.com/en/resources/kubernetes-adoption-security-market-trends-overview)) | High (C:7) |
| Early-stage (<$10M) companies default to non-K8s (supported by [Maven Solutions](https://www.mavensolutions.tech/blog/cloud-infrastructure-on-a-startup-budget), [SaaStr](https://www.saastr.com/is-there-a-benchmark-for-of-revenue-that-an-enterprise-saas-business-should-spend-on-systems-infrastructures-like-aws-or-the-equivalent/) VC guidance) | High (C:7) |
| Multi-architecture usage increases with company size (supported by [CNCF Annual Survey 2024](https://www.cncf.io/reports/cncf-annual-survey-2024/), [Flexera State of Cloud](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)) | High (C:6-7) |
| Migration direction is net toward K8s (supported by [CNCF 2025 Survey](https://www.cncf.io/announcements/2026/01/20/kubernetes-established-as-the-de-facto-operating-system-for-ai-as-production-use-hits-82-in-2025-cncf-annual-cloud-native-survey/), [Figma migration blog](https://www.figma.com/blog/migrating-onto-kubernetes/)) | Moderate (C:5) |
| EU has higher K8s and lower serverless than US (supported only by structural GDPR argument, [2 EU case studies](https://cloud.google.com/customers/stacks)) | Low (C:3) |
| Specific percentage in any cell of the estimate matrix | Low to Moderate (C:2-6, mean C:3.5) |

The gap between directional confidence (strong) and percentage confidence (weak) is the central finding of this scoring exercise. Stakeholders should rely on the directional findings and treat the specific percentages as rough order-of-magnitude estimates with +/-10-15pp uncertainty in most cells.

### What Would Raise Confidence

| Action | Cells Improved | Expected Confidence Lift |
|---|---|---|
| Direct survey of 200+ AI SaaS CTOs by ARR tier | All 21 cells | +3-5 points (most cells to C:7+) |
| [Datadog](https://www.datadoghq.com/state-of-containers-and-serverless/)/New Relic telemetry segmented for AI/ML | 15+ cells | +2-3 points |
| API-wrapper vs model-hosting split quantified | All tier cells | +1-2 points |
| EU-specific survey of 50+ AI SaaS companies | All 6 EU cells | +3-4 points |
| [CNCF survey](https://www.cncf.io/reports/cncf-annual-survey-2024/) expanded to <500 employee companies | 6 tier cells (<$10M, $10-50M) | +2-3 points |

---

**Document Version:** 1.0
**Scoring Date:** 2026-02-12
**Methodology:** Cell-by-cell adjudication of draft confidence scores against Wave 3 agent recommendations, calibrated to a 1-10 rubric
**Total Cells Scored:** 21
**Cells Reduced from Draft:** 17
**Cells Retained at Draft Level:** 4 (Managed K8s $10-50M at C:5 from draft C:6; and 3 cells already at the level recommended)
**Cells Raised from Draft:** 0
