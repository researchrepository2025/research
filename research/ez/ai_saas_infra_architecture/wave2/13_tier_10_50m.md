# Wave 2 Analysis: AI SaaS Infrastructure Architecture — $10-50M ARR Tier

**Analysis Date:** 2026-02-12
**Analyst:** Wave 2 Synthesis Agent
**Tier:** $10-50M ARR (Growth Stage)
**Wave 1 Sources Consumed:** All 8 files (01-08)

---

## Executive Summary

AI SaaS companies in the $10-50M ARR tier occupy a critical inflection point where initial architecture choices face scaling pressure and platform teams are forming. Triangulating across all 8 Wave 1 sources, **managed Kubernetes (EKS/GKE/AKS) is the dominant architecture for this tier at an estimated 50-65% adoption**, with cloud-native non-K8s services (serverless, ECS/Fargate, Cloud Run) used by 45-60% of companies, and self-managed Kubernetes used by only 5-12%. These percentages are non-exclusive -- **an estimated 40-55% of companies at this tier use multiple architectures simultaneously**, reflecting the hybrid nature of AI workloads that combine API serving, batch inference, and event-driven processing.

Infrastructure spend at this tier is severe: **20-35% of revenue** for AI SaaS vs 6-12% for traditional SaaS, creating the primary margin compression that defines this segment's economics. Engineering teams of 30-80 people typically support 3-8 infrastructure/platform engineers, which is sufficient for managed Kubernetes but insufficient for self-managed Kubernetes operations.

---

## Tier Profile: $10-50M ARR AI SaaS

### Company Characteristics

- **Stage:** Series B to Series C; some late Series A with strong growth
- **Employee count:** 80-300 employees (Estimated, based on typical SaaS headcount-to-ARR ratios of ~$200K ARR per employee at this stage)
- **Engineering team:** 30-80 engineers total
- **Infrastructure/Platform team:** 3-8 dedicated engineers
- **Customers:** 50-500 enterprise customers or 1,000-50,000 SMB customers
- **Gross margin:** 50-65% (Estimated; see Section on Infrastructure Spend below)
- **Typical funding raised:** $30-100M cumulative

### What Makes This Tier Distinct

This tier sits between two poles:
1. **Below ($1-10M ARR):** Companies typically use simple cloud-native services (serverless, PaaS, managed containers) with 1-2 infrastructure engineers. Kubernetes is generally avoided. Based on [08_vc_startup_db.md, startup infrastructure guide], which states seed-stage companies should use "fully managed platforms like Heroku, Vercel, or Firebase" and "Kubernetes should be avoided early unless absolutely necessary."
2. **Above ($50-200M ARR):** Companies have dedicated platform teams of 10-20+, run multi-region Kubernetes, and may invest in self-managed infrastructure for cost optimization. Based on [04_tech_blogs.md], which shows companies like Figma, Grammarly, and Notion operating EKS at significant scale.

The $10-50M tier is where **the migration decision happens**: companies outgrow initial serverless/PaaS architectures and must decide whether to adopt Kubernetes, optimize existing cloud-native services, or pursue a hybrid approach.

---

## Architecture Adoption Estimates

### 1. Cloud-Native Non-K8s Managed Services (Serverless, PaaS, ECS/Fargate, Cloud Run)

**Estimated Adoption: 45-60%**
**Classification: Estimated (judgment + reasoning from multiple sources)**

#### Evidence Base

**Supporting higher adoption (55-60% range):**

- Based on [02_analyst_reports.md, Data Point 13], Datadog reports "In Google Cloud, 68 percent of container organizations now use serverless containers, up from 35 percent two years ago." While this covers all GCP container users (not AI SaaS specifically), it demonstrates strong serverless container momentum.

- Based on [02_analyst_reports.md, Data Point 14], "AWS Lambda: 65% of AWS customers" and [Data Point 15] "Google Cloud Run: 70% of Google Cloud customers." These are broad adoption metrics but indicate serverless services are extremely common baseline infrastructure.

- Based on [02_analyst_reports.md, Data Point 17], "66% of organizations using serverless functions also use container orchestration," indicating serverless is often used alongside (not instead of) Kubernetes -- meaning many K8s adopters ALSO use serverless.

- Based on [08_vc_startup_db.md, startup infrastructure guide], which recommends "serverless options like AWS Lambda to reduce overhead" for early-stage companies, with Kubernetes recommended only for "funded SaaS startups targeting large-scale deployments." Companies at $10-50M ARR that started at seed stage would have initially built on serverless and may still run significant workloads there.

- Based on [05_cloud_vendor_cases.md, Case Studies 10-12], Autodesk (ECS/Fargate), BILL (ECS/Fargate, 150-200K req/min), and Smartsheet (ECS/Fargate) demonstrate SaaS companies at scale using non-K8s managed containers successfully.

**Supporting lower adoption (45-50% range):**

- Based on [01_cncf_survey.md, Data Point 31], "only 11% of respondents are making use of serverless computing frameworks" in the CNCF 2024 survey. However, this survey has strong CNCF/K8s selection bias (companies in the CNCF ecosystem are already K8s-oriented), so 11% likely underrepresents market-wide serverless adoption.

- Based on [01_cncf_survey.md, Data Point 32], serverless declined from 22% (2022) to 13% (2023) in CNCF surveys, suggesting some movement away from serverless among cloud-native practitioners.

**AI SaaS-specific considerations:**

- AI workloads often require GPU access. Based on [06_stackshare_github.md, serverless container comparison], "Google Cloud Run and Azure Container Instances both support GPU-based workloads while AWS Fargate does not." This limits pure-serverless approaches for companies running inference on their own models.

- However, many AI SaaS companies at this tier use API-based model providers (OpenAI, Anthropic APIs) rather than self-hosting models, reducing GPU infrastructure requirements and making serverless architectures more viable for the application layer.

**Confidence: 5/10 for this tier specifically.** Broad adoption data exists for serverless/managed containers, but no source directly segments $10-50M ARR AI SaaS companies. Estimate is derived from combining general market data with stage-based architecture guidance.

---

### 2. Managed Kubernetes (EKS, GKE, AKS)

**Estimated Adoption: 50-65%**
**Classification: Estimated (judgment + reasoning from multiple convergent sources)**

#### Evidence Base

**Supporting higher adoption (60-65% range):**

- Based on [01_cncf_survey.md, Data Point 1], "Production use of Kubernetes hit 80% in 2024." While this applies to all surveyed organizations (n=750), the CNCF survey skews toward cloud-native practitioners and larger orgs (91% of K8s users at 1,000+ employees per Data Point 27). Companies at $10-50M ARR with 80-300 employees are in the smaller end of K8s adopters.

- Based on [02_analyst_reports.md, Data Point 1], Dynatrace reports "73% of Kubernetes clusters in the cloud are built on top of managed distributions from the hyperscalers." Among K8s users, managed dominates overwhelmingly.

- Based on [03_job_postings.md, Data Point 4], "73% of cloud Kubernetes clusters are built on managed distributions from hyperscalers like AWS EKS, Azure AKS, or GKE."

- Based on [04_tech_blogs.md], multiple AI SaaS companies publicly disclose managed K8s usage: Grammarly (EKS), Cohere (OKE), Jasper (K8s), Contextual AI (GKE), Stacks (GKE Autopilot). The migration direction documented is consistently TOWARD managed K8s: Figma (ECS to EKS), Grammarly (EC2 to EKS).

- Based on [05_cloud_vendor_cases.md], among case studies with disclosed infrastructure, managed K8s (EKS/AKS/GKE) was used by 9 of 14 companies with visible architectures -- including AI-specific companies like Flawless (EKS), Liquid Analytics (EKS), Instabase (EKS), Sonantic (EKS), Picterra (GKE), and Stacks (GKE Autopilot).

- Based on [06_stackshare_github.md], "96% of enterprises now using the container orchestration platform" (Red Hat 2024) and managed K8s services growing at "CAGR of over 30%."

**Supporting lower adoption (50-55% range):**

- Based on [01_cncf_survey.md, Data Point 27], "only 9% of adopters are companies with 500-1,000 employees" and companies below 500 employees are not even measured. AI SaaS companies at $10-50M ARR typically have 80-300 employees, placing them in a size range that is underrepresented in K8s adoption data. K8s adoption is strongly correlated with company size.

- Based on [02_analyst_reports.md, Data Point 6], Gartner reports only "54% of survey respondents having a full or partial implementation" of Kubernetes -- significantly lower than the 80-82% CNCF figure, suggesting CNCF surveys over-count adoption. Gartner's broader enterprise base may be more representative of $10-50M ARR companies.

- Based on [03_job_postings.md, Data Point 20], 60% of DevOps positions are senior-level and only 5% are junior-level, suggesting K8s operations require experienced talent that may be hard to recruit and expensive for companies at this tier.

- Based on [08_vc_startup_db.md, startup infrastructure guide], microservices with Kubernetes is recommended for "funded SaaS startups targeting large-scale deployments" -- implying that not all funded startups reach this level.

**AI SaaS-specific considerations:**

- Based on [01_cncf_survey.md, Data Point 21], "66% of organizations hosting generative AI models use Kubernetes to manage some or all of their inference workloads." This is the most directly relevant data point -- organizations self-hosting AI models are likely to use K8s. However, many $10-50M ARR AI SaaS companies consume models via API rather than self-hosting.

- Based on [02_analyst_reports.md, Data Point 11], AI/ML model hosting splits show "37% using managed APIs, 25% self-hosting, and 13% at the edge." At $10-50M ARR, the mix likely skews toward managed APIs (lower infrastructure burden) with some self-hosting for differentiation or cost control.

**Confidence: 6/10 for this tier specifically.** Multiple convergent data points support managed K8s as the dominant orchestration choice for companies at scale. The main uncertainty is whether $10-50M ARR AI SaaS companies (which are smaller than most surveyed organizations) have adopted K8s at the same rate as larger enterprises.

---

### 3. Open/Self-Managed Kubernetes

**Estimated Adoption: 5-12%**
**Classification: Estimated (judgment + strong directional evidence)**

#### Evidence Base

**Supporting very low adoption (5-8% range):**

- Based on [02_analyst_reports.md, Data Point 2], across all environments (cloud + on-prem), "Self-managed K8s: 37%." But this includes large enterprises with dedicated operations teams. For cloud-only deployments, [Data Point 1] shows self-managed at 27%.

- Based on [01_cncf_survey.md, Data Point 19], "90% of Kubernetes users have turned to cloud-managed services" (Datadog 2021), leaving only ~10% purely self-managed. This figure likely underrepresents self-managed (Datadog customer base skews cloud-managed), but directionally supports very low self-managed adoption.

- Based on [06_stackshare_github.md], "Two out of every three Kubernetes clusters are now hosted in the cloud, up from just 45% in 2022" -- the trend is moving AWAY from self-managed.

- At $10-50M ARR, companies typically have 3-8 infrastructure engineers. Self-managed Kubernetes requires significant operational expertise: control plane management, etcd operations, security patching, network CNI configuration, and upgrade management. Based on [03_job_postings.md, Data Point 20], 60% of K8s positions require senior-level experience. A 3-8 person team cannot realistically dedicate the 2-4 full-time-equivalent engineers needed for self-managed K8s operations while also supporting application delivery.

**Supporting slightly higher adoption (8-12% range):**

- Based on [04_tech_blogs.md], some companies at significant scale run self-managed K8s: OpenAI (7,500 nodes on Azure, self-managed), Databricks (hybrid managed + self-managed), Salesforce (bare metal K8s), Elastic (with Crossplane). However, these are all $200M+ ARR companies -- much larger than the $10-50M tier.

- Some companies at $10-50M ARR may have inherited self-managed K8s from an early technical founder with strong K8s expertise, or may be running self-managed for compliance/security reasons in regulated industries.

- Based on [01_cncf_survey.md, Data Point 20], "Survey respondents were evenly split (59%) between on-premises data centers and public clouds, with both skewing heavily toward self-managed instances." This suggests self-managed remains more common than managed services among CNCF respondents -- but CNCF respondents are a highly specialized population.

**AI SaaS-specific considerations:**

- AI SaaS companies at $10-50M ARR are unlikely to run self-managed K8s unless they have exceptional infrastructure talent or specific requirements (GPU scheduling customization, on-premise deployment for enterprise customers, compliance needs).

- Based on [07_sec_earnings.md], AI SaaS companies face 40-50% COGS. Adding self-managed K8s operational overhead would further compress margins. The economic incentive strongly favors managed services at this tier.

**Confidence: 5/10 for this tier specifically.** Strong directional evidence that self-managed K8s is rare at this company size, but no data directly measures adoption rates for companies with 80-300 employees.

---

## Architecture Adoption Summary Table

| Architecture | Estimated Adoption | Classification | Confidence |
|---|---|---|---|
| Cloud-native non-K8s (serverless, PaaS, managed containers) | 45-60% | Estimated | 5/10 |
| Managed Kubernetes (EKS/GKE/AKS) | 50-65% | Estimated | 6/10 |
| Open/Self-managed Kubernetes | 5-12% | Estimated | 5/10 |

**Note:** Percentages are non-exclusive and sum to >100% because companies commonly use multiple architectures simultaneously.

---

## Multi-Architecture Usage

### Estimated Prevalence: 40-55% of companies at this tier use MULTIPLE architectures simultaneously

**Classification: Estimated (derived from convergent signals)**

#### Evidence Base

- Based on [02_analyst_reports.md, Data Point 17], "66% of organizations using serverless functions also use container orchestration." This directly demonstrates hybrid architecture as the norm, not the exception.

- Based on [01_cncf_survey.md, Data Point 9], "56% of organizations use multi-cloud solutions" with "Average of 2.8 unique cloud service providers per organization." Multi-cloud often implies multiple deployment models across those clouds.

- Based on [04_tech_blogs.md], Databricks explicitly operates "mix of self-managed and cloud-managed Kubernetes (EKS, AKS, GKE)." Elastic deploys across "AWS (4 regions), GCP (3 regions), Azure (1 region)." These large-scale examples demonstrate the multi-architecture pattern.

- Based on [05_cloud_vendor_cases.md], Coca-Cola used Azure Container Apps (non-K8s) alongside other Azure services. Flawless used EKS with Hybrid Nodes (combining cloud K8s with on-prem GPUs). These case studies show hybrid patterns at work.

**Typical multi-architecture patterns at $10-50M ARR:**

1. **K8s + Serverless:** Managed K8s for core API/inference workloads + Lambda/Cloud Functions for event-driven processing, webhooks, and background jobs. This is likely the most common hybrid pattern.

2. **K8s + Managed ML Services:** Managed K8s for application layer + SageMaker/Vertex AI for model training + Bedrock/Azure OpenAI for some inference. Based on [05_cloud_vendor_cases.md], companies like Perplexity use both SageMaker HyperPod and Bedrock alongside custom infrastructure.

3. **Serverless + Managed Containers:** Lambda for event processing + ECS/Fargate for longer-running API services. Companies that started serverless and added containerized services as they scaled.

**Confidence: 5/10.** The general trend toward hybrid architectures is well-established, but the specific prevalence at the $10-50M ARR tier for AI SaaS companies is not directly measured.

---

## Typical Architecture and Migration Patterns

### Most Common Starting Architecture

**Estimated: Serverless/PaaS + managed services (pre-$10M ARR)**
**Classification: Estimated (consistent across guidance sources)**

Based on [08_vc_startup_db.md, startup infrastructure guide]:
- Seed stage: "fully managed platforms like Heroku, Vercel, or Firebase" with "serverless options like AWS Lambda"
- Series A: "moving to structured environments (dev/stage/prod)" with "Infrastructure as Code" and "automated CI/CD pipelines"

By the time companies reach $10M ARR, they have typically been running production on serverless/PaaS for 2-4 years and are encountering scale limitations.

### Migration Patterns at $10-50M ARR

**Pattern 1: Serverless/ECS to Managed Kubernetes (Most Common)**

Based on [04_tech_blogs.md]:
- Figma migrated from ECS to EKS in "less than 12 months" driven by "cost savings, improved developer experience, and increased resiliency" and "to take advantage of the large ecosystem supported by the CNCF."
- Grammarly moved from "EC2 to EKS" for ML infrastructure, achieving "significantly reduced setup time."

**Triggers for migration (Inferred from blog posts and case studies):**
1. Need for StatefulSets, Helm charts, or CNCF ecosystem tools not available on ECS/Fargate
2. Cost optimization pressure -- K8s enables better bin-packing and autoscaling
3. GPU workload scheduling requirements
4. Multi-region deployment complexity exceeding serverless capabilities
5. Hiring market -- K8s skills more available than ECS/Fargate-specific skills

**Pattern 2: Managed Services Enhancement (Common)**

Rather than migrating to K8s, some companies add managed ML services (SageMaker, Vertex AI) alongside existing serverless/container infrastructure. This is attractive for companies whose AI workloads are primarily model training and inference that can be abstracted.

Based on [02_analyst_reports.md, Data Point 11], "37% using managed APIs" for AI/ML model hosting suggests many companies prefer abstracted AI services over self-managed infrastructure.

**Pattern 3: Stay on Serverless/PaaS (Less Common but Viable)**

Companies whose AI features are primarily API-wrapper (consuming OpenAI/Anthropic APIs rather than self-hosting models) may never need Kubernetes. Their application architecture is closer to traditional SaaS with AI features bolted on.

Based on [02_analyst_reports.md, Data Point 46], "79% of organizations are already using or experimenting with AI and machine learning PaaS services." This suggests managed AI services may reduce the need for custom K8s infrastructure.

**Migration direction:**

Based on [04_tech_blogs.md], the documented migration direction in engineering blogs is uniformly TOWARD Kubernetes:
- Figma: ECS -> EKS
- Grammarly: EC2 -> EKS
- Salesforce Data Cloud: EC2 -> K8s
- HubSpot: EC2 -> K8s

No engineering blog disclosed a migration FROM Kubernetes to serverless. This represents publication bias (companies are more likely to blog about K8s adoption than K8s abandonment), but the directional signal is strong.

**Confidence: 6/10.** Migration patterns toward K8s are well-documented for companies at/above this tier. The starting architecture estimate is based on guidance sources, not empirical measurement.

---

## Engineering Team Size and Capability

### Team Composition at $10-50M ARR

**Estimated total engineering: 30-80 engineers**
**Estimated infrastructure/platform engineers: 3-8**
**Classification: Estimated (derived from industry benchmarks)**

#### Evidence Base

- Based on [08_vc_startup_db.md, SaaS Capital 2025], "The median percent of annual recurring revenue spent on DevOps is 4%." At $10-50M ARR, that translates to $400K-$2M annually on DevOps, which supports approximately 2-10 DevOps/platform engineers depending on seniority and location.

- Based on [03_job_postings.md, Data Point 3], platform engineers represent 8.35% of infrastructure job postings (2024), growing to 11% (Q1 2025). This indicates companies at scale are specifically hiring platform engineers, distinct from general DevOps.

- Based on [03_job_postings.md, Data Point 16], platform engineer compensation averages "$170,657" (20% higher than general DevOps at $141,645). At $10-50M ARR, a team of 3-8 platform engineers costs $500K-$1.4M in compensation alone.

### Can This Tier Run Self-Managed Kubernetes?

**Assessment: Generally NO, with narrow exceptions**

**Arguments against:**

1. **Team size constraint:** Self-managed K8s requires 2-4 dedicated operations engineers just for the control plane, etcd, networking, security patching, and upgrades. Based on [04_tech_blogs.md], OpenAI scaled to 7,500 nodes -- but OpenAI had hundreds of infrastructure engineers. A 3-8 person platform team cannot afford to dedicate 2-4 engineers purely to K8s operations.

2. **Expertise scarcity:** Based on [03_job_postings.md, Data Point 20], 60% of K8s positions require senior-level experience. Recruiting multiple senior K8s operations engineers at $10-50M ARR is expensive and competes with larger companies offering higher compensation.

3. **Opportunity cost:** Based on [08_vc_startup_db.md, platform engineering research], "71% of leading adopters of platform engineering indicated they have significantly accelerated their time to market." Platform engineers at this tier should focus on developer experience and deployment automation, not control plane operations.

4. **Economics:** Based on [07_sec_earnings.md], AI SaaS companies already face 40-50% COGS. Adding operational overhead for self-managed K8s further compresses margins without clear revenue benefit.

**Narrow exceptions where self-managed may make sense:**
- Companies with exceptionally strong founding technical teams from hyperscaler backgrounds
- Companies requiring custom GPU scheduling that managed K8s does not support
- Companies deploying on-premises for regulated enterprise customers
- Companies operating at the very top of this tier ($40-50M ARR) with larger platform teams (8-12)

**Confidence: 7/10.** Team size constraints and economics strongly favor managed services at this tier. The rare exceptions are real but apply to a small minority.

---

## Infrastructure Spend as Percentage of Revenue

### Estimated: 20-35% of revenue for AI SaaS at $10-50M ARR

**Classification: Estimated (triangulated from multiple sources, interpolated for this tier)**

#### Evidence Base

**Traditional SaaS benchmarks (lower bound context):**

- Based on [07_sec_earnings.md], "For a typical SaaS company, cloud hosting costs usually account for 6%-12% of SaaS revenue."
- Based on [08_vc_startup_db.md], SaaStr reports "roughly 5% of revenues for 'compute-lite' companies and around 8-10% on average" for traditional SaaS.
- Based on [07_sec_earnings.md], traditional SaaS achieves 70-85% gross margins with 15-30% COGS.

**AI SaaS benchmarks (full range):**

- Based on [07_sec_earnings.md], "an AI SaaS might see 40-50% (or more) of revenue eaten by COGS in the form of model hosting, inference compute, and data costs." This applies broadly to AI-first SaaS.
- Based on [08_vc_startup_db.md], "Compute costs for AI companies went from 24% of revenue to 50% of revenue (versus non-AI SaaS where it more or less stayed at about 18%)."
- Based on [08_vc_startup_db.md], "For AI startups, compute costs as a percentage of revenue is a critical metric, with AI startups often starting much higher than traditional software companies - sometimes in the 50-80% range."
- Based on [08_vc_startup_db.md], Bessemer found "fast-scaling AI SaaS startups had only ~25% gross margin on average in early stages, while even steadier-growth AI companies managed around 60% gross margin."

**Tier-specific interpolation:**

At $10-50M ARR, companies are past the earliest stages (where compute costs can be 50-80% of revenue due to low revenue base) but have not yet achieved the scale economies of $100M+ ARR companies. The 20-35% estimate reflects:

1. Companies that primarily consume AI APIs (OpenAI, Anthropic) rather than self-hosting -- lower infrastructure costs, closer to 15-25% of revenue
2. Companies that self-host models for inference -- higher infrastructure costs, 30-50% of revenue
3. The mix: most companies at this tier use a blend of API consumption and some self-hosted inference

Based on [08_vc_startup_db.md], one optimization example showed "a client spending $45K monthly on AWS for 500 customers reduced costs to $28K monthly after optimization, dropping infrastructure from 18% of revenue to 11%." This traditional SaaS example suggests AI SaaS infrastructure optimization at this tier could reduce costs from 30-35% toward 20-25%.

**Gross margin implications:**

With infrastructure at 20-35% of revenue (plus other COGS: support, data, third-party services), gross margins at this tier are estimated at:
- API-heavy AI SaaS: 55-65% gross margin
- Model-hosting AI SaaS: 45-55% gross margin
- Target for this tier: 50-65% gross margin

Based on [07_sec_earnings.md], "AI-first B2B SaaS companies achieve gross margins of 50-65% due to high inference and infrastructure costs, compared to 80-90% for traditional SaaS." This aligns with the estimated range.

**Confidence: 6/10.** Infrastructure cost ranges for AI SaaS are well-documented, but the specific 20-35% estimate for the $10-50M ARR tier is interpolated rather than directly measured.

---

## Decision Drivers for Architecture Choice

### Ranked by Importance at $10-50M ARR (Estimated)

#### 1. Engineering Team Velocity (Highest Priority)

At this tier, shipping features and closing enterprise deals is existential. Architecture choices that accelerate developer productivity take priority.

- Based on [04_tech_blogs.md], Figma cited "improved developer experience" as a key driver for ECS-to-K8s migration.
- Based on [08_vc_startup_db.md], "71% of leading adopters of platform engineering indicated they have significantly accelerated their time to market."
- Based on [04_tech_blogs.md], Jasper built "JasperCLI" to give engineers "ephemeral testing environments" and seamless K8s interaction -- prioritizing developer experience over infrastructure sophistication.

**Implication:** Companies choose managed K8s or serverless primarily based on which enables faster developer iteration, not theoretical cost optimization.

#### 2. Cost and Margin Pressure (High Priority)

- Based on [07_sec_earnings.md], AI SaaS gross margins (50-65%) are significantly below traditional SaaS (70-85%). Infrastructure is the primary margin lever.
- Based on [08_vc_startup_db.md], "89% of CFOs report that rising cloud costs have negatively impacted gross margins over the past 12 months."
- Based on [07_sec_earnings.md], GPU spending grew 40% as % of EC2 costs from 10% to 14% in one year, and "83% of container costs were associated with idle resources."
- Based on [08_vc_startup_db.md], CAST AI and Komodor's rapid growth (200% and 400% YoY respectively) indicates strong market demand for K8s cost optimization.

**Implication:** Companies adopt K8s for better bin-packing and autoscaling to control GPU and compute costs. Based on [04_tech_blogs.md], Figma cited "cost savings by not over-provisioning for deploys" as a migration benefit.

#### 3. Scalability and Reliability (High Priority)

- At $10-50M ARR, companies are serving real enterprise customers with SLA requirements.
- Based on [04_tech_blogs.md], Figma achieved "improved reliability through multi-cluster deployment" after moving to K8s.
- Based on [05_cloud_vendor_cases.md], Liquid Analytics achieved "1,000 Kubernetes pods launched in 1.2 seconds" on EKS -- demonstrating rapid scaling capability.
- Based on [02_analyst_reports.md, Data Point 41], "64% of Kubernetes organizations use Horizontal Pod Autoscaler" -- autoscaling is mainstream on K8s.

**Implication:** K8s is chosen when serverless scaling limits (cold starts, timeout limits, connection pooling) become bottlenecks for the application.

#### 4. GPU Access and AI Workload Requirements (Medium-High Priority)

- Based on [06_stackshare_github.md, serverless container comparison], "AWS Fargate does not support GPU-based workloads" while Cloud Run and Azure Container Instances do. This fundamental limitation drives some companies from Fargate toward K8s for inference.
- Based on [04_tech_blogs.md], CNCF launched the "Certified Kubernetes AI Conformance program" to standardize "GPU management, networking, and gang scheduling" on K8s.
- Based on [08_vc_startup_db.md], GPU availability even drove cloud provider selection: a YC founder noted "Azure was the only place where they could find GPUs."

**Implication:** Companies self-hosting GPU inference workloads are pushed toward K8s. Companies consuming inference via API avoid this pressure.

#### 5. Ecosystem and Tooling (Medium Priority)

- Based on [04_tech_blogs.md], Figma migrated to K8s "primarily to take advantage of the large ecosystem supported by the CNCF" -- ECS lacked "support for StatefulSets, Helm charts, and inability to easily run OSS software like Temporal."
- Based on [06_stackshare_github.md], "39,816 verified companies are using Kubernetes" -- the ecosystem effect creates a talent and tooling advantage.
- K8s-native tools (ArgoCD, Helm, Karpenter, Prometheus) create an integrated platform experience unavailable on serverless.

**Implication:** Companies choose K8s when they need CNCF ecosystem tools that are unavailable or poorly supported on serverless platforms.

#### 6. Compliance and Security (Medium Priority)

- Based on [01_cncf_survey.md, Data Point 13], "40% cite security as the leading container challenge."
- Enterprise sales at this tier often require SOC 2, HIPAA, or FedRAMP compliance, which can be easier to demonstrate on K8s with established tooling (Falco, OPA, network policies) than on serverless.
- Based on [05_cloud_vendor_cases.md, Case Study 4], Sonantic achieved "100% compliance score in AWS Security Hub" on EKS.

**Implication:** Compliance requirements are a tiebreaker that can push companies toward K8s, but are rarely the primary driver at this tier.

#### 7. Multi-Cloud / Portability (Low Priority at This Tier)

- Based on [02_analyst_reports.md, Data Point 30], "65% of organizations run Kubernetes in multiple environments for portability." However, this is primarily an enterprise (1,000+ employee) concern.
- At $10-50M ARR, most companies are single-cloud. Multi-cloud is a future concern, not a current driver.
- Based on [01_cncf_survey.md, Data Point 11], "Small organizations: 27% hybrid cloud adoption" vs 56% for large organizations.

**Implication:** Portability is aspirational at this tier but rarely drives architecture decisions today.

---

## Evidence Gaps

### Critical Gaps (Data Does Not Exist in Wave 1 Sources)

1. **Direct adoption survey for $10-50M ARR AI SaaS:** No Wave 1 source segments by revenue tier. All adoption data is by employee count (biased toward large enterprises), cloud-native maturity, or general market categories. The core research question cannot be answered with Direct data -- all estimates are Estimated or Inferred.

2. **Infrastructure cost as % of revenue by tier:** Based on [07_sec_earnings.md], SEC data represents "exclusively large-cap public companies ($200M+ ARR)." The 20-35% estimate for $10-50M ARR is interpolated, not measured.

3. **Migration trigger points:** No data exists on at what ARR or scale companies typically migrate from serverless to K8s. The $10-50M ARR threshold is inferred from blog posts and guidance documents.

4. **Team size for K8s operations by company size:** No source measures how many engineers companies at this tier dedicate to infrastructure. The 3-8 estimate is derived from spending benchmarks and headcount ratios.

5. **Self-managed K8s adoption at <500 employees:** Based on [01_cncf_survey.md, Data Point 27] and [03_job_postings.md, Data Point 7], K8s adoption data stops at 500-1,000 employees. Companies at $10-50M ARR (80-300 employees) are below the measurement threshold.

6. **Failure/abandonment rates:** Based on [04_tech_blogs.md, Limitations], "No companies disclosed failed K8s migrations or decisions to abandon K8s." Publication bias means we see only successes.

7. **API-wrapper vs self-hosted model split:** No data quantifies what percentage of AI SaaS companies at this tier consume AI APIs vs self-host models. This split fundamentally changes infrastructure requirements.

### Moderate Gaps (Partial Data Available)

8. **Serverless adoption specifically for AI inference:** Based on [01_cncf_survey.md, Data Point 22], "44% of organizations do not yet run AI/ML workloads on Kubernetes." But it is unclear what those 44% use instead -- serverless? Managed ML services? No containers at all?

9. **Cost comparison between architectures:** No source provides empirical TCO comparison between managed K8s vs serverless vs self-managed K8s for AI SaaS workloads at this tier.

10. **Geographic variation:** Based on [01_cncf_survey.md, Data Point 15], cloud-native adoption varies significantly by region (Europe 82%, Americas 70%, APAC 40%). Architecture choices at $10-50M ARR likely vary by region, but no data exists.

---

## Assumptions Register

| # | Assumption | Basis | Impact if Wrong |
|---|---|---|---|
| A1 | $10-50M ARR AI SaaS companies have 80-300 employees | Industry benchmark of ~$200K ARR/employee for growth-stage SaaS | Team size estimates and K8s feasibility assessment would change |
| A2 | 3-8 engineers on infrastructure/platform teams | Derived from 4% DevOps spending benchmark (SaaS Capital) and typical team structures | Self-managed K8s feasibility assessment would change |
| A3 | Companies that started before 2023 initially built on serverless/PaaS | Based on startup infrastructure guidance in [08_vc_startup_db.md] | Migration pattern analysis would change if many started on K8s |
| A4 | CNCF survey data overestimates K8s adoption for this tier | CNCF respondents are cloud-native practitioners; 91% of K8s users at 1,000+ employee orgs | All K8s adoption estimates would shift upward if CNCF data is representative |
| A5 | Managed K8s (EKS/GKE/AKS) costs less in operational overhead than self-managed K8s for teams of <10 infrastructure engineers | Based on managed service value proposition and team size constraints | Self-managed K8s assessment would change |
| A6 | AI SaaS companies at this tier use a mix of API-consumed models and self-hosted inference | No direct data; inferred from AI/ML hosting splits in [02_analyst_reports.md, Data Point 11] | Infrastructure spend and architecture estimates would change significantly |
| A7 | Companies at this tier are primarily single-cloud | Based on [01_cncf_survey.md, Data Point 11] showing small orgs at 27% hybrid adoption | Multi-architecture prevalence estimate would change |
| A8 | Engineering blogs in [04_tech_blogs.md] are representative of architecture patterns despite publication bias | Companies that blog tend to be larger and more mature | If non-blogging companies differ significantly, all qualitative architecture patterns would be wrong |
| A9 | GPU-requiring AI workloads push companies toward K8s over serverless | Based on Fargate GPU limitation and K8s GPU scheduling capabilities | If managed ML services (SageMaker, Vertex AI) satisfy GPU needs, K8s adoption could be lower |
| A10 | Infrastructure spend at 20-35% of revenue is interpolated between early-stage (50-80%) and mature ($100M+ ARR) ranges | Linear interpolation; actual cost curve may be non-linear | Cost estimates could be significantly higher or lower |

---

## Synthesis: What Is Typical at $10-50M ARR?

### The Modal Company

The most typical AI SaaS company at $10-50M ARR likely:

1. **Runs managed Kubernetes (EKS or GKE) for core application and inference workloads**, having migrated from simpler infrastructure (ECS, EC2, or PaaS) during Series A or B.

2. **Also uses serverless services (Lambda, Cloud Functions) for event-driven processing**, webhooks, asynchronous jobs, and lightweight API endpoints -- making them a hybrid architecture user.

3. **Consumes AI model APIs (OpenAI, Anthropic, Google) for some inference** while self-hosting differentiated models on K8s with GPUs for others.

4. **Has a platform team of 4-6 engineers** managing Terraform/IaC, CI/CD (GitHub Actions or GitLab), and K8s operations using managed services and CNCF tooling (ArgoCD, Helm, Karpenter, Prometheus).

5. **Spends 20-30% of revenue on infrastructure**, achieving 55-65% gross margins, and is actively optimizing costs through spot instances, autoscaling, and right-sizing.

6. **Does NOT run self-managed Kubernetes** -- the team is too small and the economic pressure too high to justify control plane operations.

7. **Is single-cloud (AWS most likely)** with all production workloads on one provider, using cloud credits negotiated during fundraising.

### Variation Within the Tier

**Lower end ($10-20M ARR):**
- More likely still on serverless/ECS, evaluating K8s migration
- Smaller platform team (2-4 engineers)
- Higher infrastructure spend as % of revenue (25-35%)
- More reliance on managed AI APIs
- May still use PaaS (Render, Railway) for non-critical services

**Upper end ($30-50M ARR):**
- Almost certainly on managed K8s for core workloads
- Larger platform team (5-8 engineers)
- Infrastructure spend optimization underway (20-28% of revenue)
- More self-hosted inference for cost control
- Beginning to invest in internal developer platform (IDP)
- May be evaluating multi-region deployment

---

## Confidence Assessment

### Overall Confidence: 5/10

**Why not higher:**
- No Wave 1 source directly measures architecture adoption for $10-50M ARR AI SaaS companies
- All tier-specific estimates are Estimated (judgment + reasoning), not Direct or Inferred
- Strong biases in all data sources: CNCF over-counts K8s, vendor case studies over-count their own products, blog posts over-represent successful migrations, SEC filings only cover $200M+ ARR companies
- Company size segmentation stops at 500 employees -- below the typical headcount for this tier
- The critical distinction between "AI-wrapper SaaS" (consuming APIs) and "AI-native SaaS" (self-hosting models) is not captured in any data source, yet fundamentally changes infrastructure requirements

**Why not lower:**
- Multiple independent data sources converge on the same directional conclusions: managed K8s dominant, self-managed K8s rare, serverless widely used alongside K8s
- Engineering blog case studies from companies near this tier (Figma, Grammarly, Jasper, Notion, Stacks) provide qualitative validation
- Infrastructure cost ranges (20-35% for AI SaaS) are well-triangulated across VC reports, analyst data, and SEC filings
- The constraints (team size, cost pressure, hiring market) that drive architecture decisions are well-documented even if specific adoption rates are not

### Estimate Reliability Ranking

| Estimate | Reliability | Rationale |
|---|---|---|
| Self-managed K8s is rare (5-12%) | HIGH | Multiple converging signals: team size, economics, market trend all point same direction |
| Infrastructure spend 20-35% of revenue | MEDIUM-HIGH | Well-triangulated across multiple sources, though interpolated for this tier |
| Managed K8s dominant (50-65%) | MEDIUM | Strong directional evidence but uncertain exactly how much lower adoption is at this company size vs large enterprise |
| Multi-architecture common (40-55%) | MEDIUM | Hybrid patterns well-documented in general; prevalence at this tier uncertain |
| Cloud-native non-K8s (45-60%) | MEDIUM-LOW | Broad serverless adoption data exists but conflicting signals (CNCF 11% vs Datadog 65-70%) |
| Team size 3-8 platform engineers | LOW-MEDIUM | Derived from spending benchmarks, not direct measurement |

---

## Sources Consumed

All 8 Wave 1 files were read in full:

1. **01_cncf_survey.md** — 40 data points on K8s adoption, managed vs self-managed splits, AI/ML workload patterns, company size segmentation
2. **02_analyst_reports.md** — 46 data points from Gartner, Flexera, Datadog, HashiCorp on K8s, serverless, AI/ML deployment patterns
3. **03_job_postings.md** — 20 data points on K8s job market, role distribution, company size, salary data
4. **04_tech_blogs.md** — 15+ company case studies on K8s architecture, migration patterns, scale metrics
5. **05_cloud_vendor_cases.md** — 25 vendor case studies from AWS, Azure, GCP with architecture disclosures
6. **06_stackshare_github.md** — Market statistics on K8s, serverless, container adoption rates, version adoption
7. **07_sec_earnings.md** — Public company financial data, cloud commitments, infrastructure cost benchmarks
8. **08_vc_startup_db.md** — VC investment trends, startup infrastructure spending, gross margin benchmarks, platform engineering adoption
